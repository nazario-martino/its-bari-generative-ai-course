{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔢 Notebook 3: Word Embeddings - Rappresentare le Parole come Numeri\n",
    "\n",
    "**Trasformiamo le parole in vettori!** 🧮\n",
    "\n",
    "## 🎯 Obiettivi di questo notebook:\n",
    "- Comprendere perché l'AI ha bisogno di numeri\n",
    "- Implementare Bag of Words (BoW)\n",
    "- Calcolare TF-IDF da zero\n",
    "- Esplorare Word2Vec pre-addestrato\n",
    "- Visualizzare embeddings con t-SNE\n",
    "- Confrontare diversi metodi di rappresentazione\n",
    "- Applicare embeddings a problemi reali\n",
    "\n",
    "## ⏱️ Tempo stimato: 75-90 minuti\n",
    "\n",
    "## 📋 Prerequisiti: \n",
    "- Notebook 1 e 2 completati\n",
    "- Concetti base di algebra lineare (vettori, distanze)\n",
    "- Familiarità con la pre-elaborazione del testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installiamo le librerie necessarie\n",
    "!pip install gensim scikit-learn matplotlib seaborn pandas numpy nltk spacy wordcloud plotly\n",
    "!pip install umap-learn  # Per visualizzazioni alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Setup e Installazione\n",
    "\n",
    "Installiamo le librerie necessarie per lavorare con gli embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Librerie per NLP\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importiamo le librerie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Librerie per NLP\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Librerie per embeddings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Visualizzazioni\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Download dati NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Configurazione grafica\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Setup completato! Pronti per esplorare i Word Embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Perché l'AI Ha Bisogno di Numeri?\n",
    "\n",
    "I computer non capiscono le parole, ma solo numeri. Vediamo il problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il problema della rappresentazione\n",
    "print(\"🤖 IL PROBLEMA: Come fa un computer a capire le parole?\\n\")\n",
    "\n",
    "# Esempio di parole\n",
    "parole = [\"gatto\", \"cane\", \"automobile\", \"felice\", \"triste\"]\n",
    "\n",
    "print(\"📝 PAROLE UMANE:\")\n",
    "for i, parola in enumerate(parole):\n",
    "    print(f\"   {i+1}. {parola}\")\n",
    "\n",
    "print(\"\\n🔢 RAPPRESENTAZIONE COMPUTER (ASCII):\")\n",
    "for i, parola in enumerate(parole):\n",
    "    ascii_vals = [ord(char) for char in parola]\n",
    "    print(f\"   {i+1}. {parola} → {ascii_vals}\")\n",
    "\n",
    "print(\"\\n❌ PROBLEMI CON ASCII:\")\n",
    "print(\"   • Lunghezze diverse\")\n",
    "print(\"   • Nessuna relazione semantica\")\n",
    "print(\"   • 'gatto' e 'cane' sembrano completamente diversi\")\n",
    "print(\"   • Non cattura il significato\")\n",
    "\n",
    "# Visualizziamo il problema\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Grafico 1: Lunghezze parole\n",
    "lunghezze = [len(parola) for parola in parole]\n",
    "ax1.bar(parole, lunghezze, color='lightcoral', alpha=0.7)\n",
    "ax1.set_title('Lunghezze Parole (Caratteri)')\n",
    "ax1.set_ylabel('Numero Caratteri')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Grafico 2: Somma valori ASCII\n",
    "ascii_sums = [sum(ord(char) for char in parola) for parola in parole]\n",
    "ax2.bar(parole, ascii_sums, color='lightblue', alpha=0.7)\n",
    "ax2.set_title('Somma Valori ASCII')\n",
    "ax2.set_ylabel('Somma ASCII')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 SOLUZIONE: Word Embeddings!\")\n",
    "print(\"   • Rappresentano parole come vettori di numeri\")\n",
    "print(\"   • Catturano relazioni semantiche\")\n",
    "print(\"   • Parole simili hanno vettori simili\")\n",
    "print(\"   • Dimensione fissa per tutte le parole\")\n",
    "\n",
    "# Esempio concettuale di embeddings\n",
    "print(\"\\n🎯 ESEMPIO CONCETTUALE (2D):\")\n",
    "embeddings_esempio = {\n",
    "    'gatto': [0.8, 0.9],    # Animale domestico\n",
    "    'cane': [0.9, 0.8],     # Animale domestico (simile a gatto)\n",
    "    'automobile': [-0.5, 0.1],  # Oggetto meccanico\n",
    "    'felice': [0.2, -0.8],  # Emozione positiva\n",
    "    'triste': [0.1, 0.7]    # Emozione negativa\n",
    "}\n",
    "\n",
    "# Visualizziamo gli embeddings concettuali\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for parola, (x, y) in embeddings_esempio.items():\n",
    "    ax.scatter(x, y, s=200, alpha=0.7)\n",
    "    ax.annotate(parola, (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Dimensione 1 (es. \"Animale vs Oggetto\")')\n",
    "ax.set_ylabel('Dimensione 2 (es. \"Domestico vs Selvaggio\")')\n",
    "ax.set_title('Embeddings Concettuali 2D')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"👀 OSSERVAZIONI:\")\n",
    "print(\"   • 'gatto' e 'cane' sono vicini (entrambi animali domestici)\")\n",
    "print(\"   • 'automobile' è lontano (oggetto meccanico)\")\n",
    "print(\"   • 'felice' e 'triste' sono in zone diverse (emozioni opposte)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎒 Bag of Words (BoW) - Il Metodo Base\n",
    "\n",
    "Iniziamo con il metodo più semplice: **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementiamo Bag of Words da zero\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabolario = {}\n",
    "        self.vocabolario_inverso = {}\n",
    "        self.dimensione_vocab = 0\n",
    "    \n",
    "    def costruisci_vocabolario(self, documenti):\n",
    "        \"\"\"Costruisce il vocabolario da una lista di documenti\"\"\"\n",
    "        tutte_parole = set()\n",
    "        \n",
    "        for doc in documenti:\n",
    "            parole = doc.lower().split()\n",
    "            tutte_parole.update(parole)\n",
    "        \n",
    "        # Crea mappatura parola -> indice\n",
    "        for i, parola in enumerate(sorted(tutte_parole)):\n",
    "            self.vocabolario[parola] = i\n",
    "            self.vocabolario_inverso[i] = parola\n",
    "        \n",
    "        self.dimensione_vocab = len(self.vocabolario)\n",
    "        print(f\"📚 Vocabolario costruito: {self.dimensione_vocab} parole uniche\")\n",
    "    \n",
    "    def documento_to_vettore(self, documento):\n",
    "        \"\"\"Converte un documento in vettore BoW\"\"\"\n",
    "        vettore = np.zeros(self.dimensione_vocab)\n",
    "        parole = documento.lower().split()\n",
    "        \n",
    "        for parola in parole:\n",
    "            if parola in self.vocabolario:\n",
    "                indice = self.vocabolario[parola]\n",
    "                vettore[indice] += 1\n",
    "        \n",
    "        return vettore\n",
    "    \n",
    "    def trasforma_documenti(self, documenti):\n",
    "        \"\"\"Trasforma una lista di documenti in matrice BoW\"\"\"\n",
    "        matrice = []\n",
    "        for doc in documenti:\n",
    "            vettore = self.documento_to_vettore(doc)\n",
    "            matrice.append(vettore)\n",
    "        return np.array(matrice)\n",
    "\n",
    "# Test con documenti di esempio\n",
    "print(\"🎒 BAG OF WORDS - IMPLEMENTAZIONE DA ZERO\\n\")\n",
    "\n",
    "documenti_esempio = [\n",
    "    \"il gatto dorme sul divano\",\n",
    "    \"il cane corre nel parco\",\n",
    "    \"il gatto e il cane sono amici\",\n",
    "    \"sul divano c'è un gatto nero\",\n",
    "    \"nel parco corrono molti cani\"\n",
    "]\n",
    "\n",
    "print(\"📝 DOCUMENTI DI ESEMPIO:\")\n",
    "for i, doc in enumerate(documenti_esempio, 1):\n",
    "    print(f\"   {i}. {doc}\")\n",
    "\n",
    "# Costruiamo il modello BoW\n",
    "bow = BagOfWords()\n",
    "bow.costruisci_vocabolario(documenti_esempio)\n",
    "\n",
    "print(f\"\\n📚 VOCABOLARIO ({bow.dimensione_vocab} parole):\")\n",
    "print(list(bow.vocabolario.keys()))\n",
    "\n",
    "# Trasformiamo i documenti\n",
    "matrice_bow = bow.trasforma_documenti(documenti_esempio)\n",
    "\n",
    "print(f\"\\n🔢 MATRICE BAG OF WORDS ({matrice_bow.shape[0]} documenti x {matrice_bow.shape[1]} parole):\")\n",
    "\n",
    "# Creiamo un DataFrame per visualizzare meglio\n",
    "df_bow = pd.DataFrame(matrice_bow, \n",
    "                      columns=list(bow.vocabolario.keys()),\n",
    "                      index=[f\"Doc {i+1}\" for i in range(len(documenti_esempio))])\n",
    "\n",
    "print(df_bow)\n",
    "\n",
    "# Visualizzazione della matrice\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap della matrice BoW\n",
    "sns.heatmap(df_bow, annot=True, cmap='Blues', ax=ax1, cbar_kws={'label': 'Frequenza'})\n",
    "ax1.set_title('Matrice Bag of Words')\n",
    "ax1.set_xlabel('Parole')\n",
    "ax1.set_ylabel('Documenti')\n",
    "\n",
    "# Frequenze totali delle parole\n",
    "freq_parole = matrice_bow.sum(axis=0)\n",
    "parole_ordinate = sorted(bow.vocabolario.keys(), key=lambda x: freq_parole[bow.vocabolario[x]], reverse=True)\n",
    "freq_ordinate = [freq_parole[bow.vocabolario[p]] for p in parole_ordinate[:10]]\n",
    "\n",
    "ax2.bar(parole_ordinate[:10], freq_ordinate, color='lightblue', alpha=0.7)\n",
    "ax2.set_title('Top 10 Parole Più Frequenti')\n",
    "ax2.set_xlabel('Parole')\n",
    "ax2.set_ylabel('Frequenza Totale')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 CARATTERISTICHE BAG OF WORDS:\")\n",
    "print(\"✅ Vantaggi:\")\n",
    "print(\"   • Semplice da implementare\")\n",
    "print(\"   • Interpretabile\")\n",
    "print(\"   • Veloce\")\n",
    "print(\"   • Buono per classificazione base\")\n",
    "\n",
    "print(\"\\n❌ Svantaggi:\")\n",
    "print(\"   • Ignora l'ordine delle parole\")\n",
    "print(\"   • Vettori molto sparsi (molti zeri)\")\n",
    "print(\"   • Dimensionalità alta\")\n",
    "print(\"   • Non cattura relazioni semantiche\")\n",
    "\n",
    "# Esempio di similarità\n",
    "print(\"\\n🔍 ESEMPIO SIMILARITÀ:\")\n",
    "similarita = cosine_similarity(matrice_bow)\n",
    "print(f\"Similarità Doc 1 vs Doc 3: {similarita[0][2]:.3f}\")\n",
    "print(f\"Similarità Doc 1 vs Doc 4: {similarita[0][3]:.3f}\")\n",
    "print(\"(Doc 1 e 4 parlano entrambi di gatti sul divano)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 TF-IDF - Pesare l'Importanza delle Parole\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) migliora BoW pesando l'importanza delle parole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementiamo TF-IDF da zero\n",
    "class TFIDF:\n",
    "    def __init__(self):\n",
    "        self.vocabolario = {}\n",
    "        self.idf_scores = {}\n",
    "        self.dimensione_vocab = 0\n",
    "    \n",
    "    def calcola_tf(self, documento):\n",
    "        \"\"\"Calcola Term Frequency per un documento\"\"\"\n",
    "        parole = documento.lower().split()\n",
    "        tf = Counter(parole)\n",
    "        \n",
    "        # Normalizza per la lunghezza del documento\n",
    "        lunghezza_doc = len(parole)\n",
    "        for parola in tf:\n",
    "            tf[parola] = tf[parola] / lunghezza_doc\n",
    "        \n",
    "        return tf\n",
    "    \n",
    "    def calcola_idf(self, documenti):\n",
    "        \"\"\"Calcola Inverse Document Frequency\"\"\"\n",
    "        n_documenti = len(documenti)\n",
    "        \n",
    "        # Conta in quanti documenti appare ogni parola\n",
    "        df = defaultdict(int)  # document frequency\n",
    "        \n",
    "        for doc in documenti:\n",
    "            parole_uniche = set(doc.lower().split())\n",
    "            for parola in parole_uniche:\n",
    "                df[parola] += 1\n",
    "        \n",
    "        # Calcola IDF\n",
    "        for parola, freq_doc in df.items():\n",
    "            self.idf_scores[parola] = math.log(n_documenti / freq_doc)\n",
    "        \n",
    "        # Costruisci vocabolario\n",
    "        for i, parola in enumerate(sorted(df.keys())):\n",
    "            self.vocabolario[parola] = i\n",
    "        \n",
    "        self.dimensione_vocab = len(self.vocabolario)\n",
    "    \n",
    "    def documento_to_tfidf(self, documento):\n",
    "        \"\"\"Converte documento in vettore TF-IDF\"\"\"\n",
    "        tf = self.calcola_tf(documento)\n",
    "        vettore = np.zeros(self.dimensione_vocab)\n",
    "        \n",
    "        for parola, tf_score in tf.items():\n",
    "            if parola in self.vocabolario:\n",
    "                indice = self.vocabolario[parola]\n",
    "                idf_score = self.idf_scores[parola]\n",
    "                vettore[indice] = tf_score * idf_score\n",
    "        \n",
    "        return vettore\n",
    "    \n",
    "    def fit_transform(self, documenti):\n",
    "        \"\"\"Addestra e trasforma i documenti\"\"\"\n",
    "        self.calcola_idf(documenti)\n",
    "        \n",
    "        matrice = []\n",
    "        for doc in documenti:\n",
    "            vettore = self.documento_to_tfidf(doc)\n",
    "            matrice.append(vettore)\n",
    "        \n",
    "        return np.array(matrice)\n",
    "\n",
    "# Test TF-IDF\n",
    "print(\"📊 TF-IDF - IMPLEMENTAZIONE DA ZERO\\n\")\n",
    "\n",
    "# Usiamo gli stessi documenti\n",
    "tfidf = TFIDF()\n",
    "matrice_tfidf = tfidf.fit_transform(documenti_esempio)\n",
    "\n",
    "print(f\"📚 Vocabolario TF-IDF: {tfidf.dimensione_vocab} parole\")\n",
    "\n",
    "# Mostriamo i punteggi IDF\n",
    "print(\"\\n🔢 PUNTEGGI IDF (Inverse Document Frequency):\")\n",
    "idf_ordinati = sorted(tfidf.idf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for parola, idf in idf_ordinati:\n",
    "    print(f\"   '{parola}': {idf:.3f}\")\n",
    "\n",
    "print(\"\\n💡 INTERPRETAZIONE IDF:\")\n",
    "print(\"   • Punteggio ALTO = parola rara (appare in pochi documenti)\")\n",
    "print(\"   • Punteggio BASSO = parola comune (appare in molti documenti)\")\n",
    "print(\"   • 'il' ha IDF basso perché appare ovunque\")\n",
    "print(\"   • 'nero' ha IDF alto perché appare solo in un documento\")\n",
    "\n",
    "# Confronto BoW vs TF-IDF\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# DataFrame per TF-IDF\n",
    "df_tfidf = pd.DataFrame(matrice_tfidf, \n",
    "                        columns=list(tfidf.vocabolario.keys()),\n",
    "                        index=[f\"Doc {i+1}\" for i in range(len(documenti_esempio))])\n",
    "\n",
    "# Heatmap BoW\n",
    "sns.heatmap(df_bow, annot=True, cmap='Blues', ax=axes[0,0], \n",
    "            cbar_kws={'label': 'Frequenza'})\n",
    "axes[0,0].set_title('Bag of Words')\n",
    "\n",
    "# Heatmap TF-IDF\n",
    "sns.heatmap(df_tfidf, annot=True, fmt='.2f', cmap='Reds', ax=axes[0,1],\n",
    "            cbar_kws={'label': 'TF-IDF Score'})\n",
    "axes[0,1].set_title('TF-IDF')\n",
    "\n",
    "# Confronto punteggi per una parola specifica\n",
    "parola_esempio = 'gatto'\n",
    "if parola_esempio in bow.vocabolario and parola_esempio in tfidf.vocabolario:\n",
    "    idx_bow = bow.vocabolario[parola_esempio]\n",
    "    idx_tfidf = tfidf.vocabolario[parola_esempio]\n",
    "    \n",
    "    bow_scores = matrice_bow[:, idx_bow]\n",
    "    tfidf_scores = matrice_tfidf[:, idx_tfidf]\n",
    "    \n",
    "    x = np.arange(len(documenti_esempio))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,0].bar(x - width/2, bow_scores, width, label='BoW', alpha=0.7, color='blue')\n",
    "    axes[1,0].bar(x + width/2, tfidf_scores, width, label='TF-IDF', alpha=0.7, color='red')\n",
    "    axes[1,0].set_title(f'Confronto Punteggi: \"{parola_esempio}\"')\n",
    "    axes[1,0].set_xlabel('Documenti')\n",
    "    axes[1,0].set_ylabel('Punteggio')\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels([f'Doc {i+1}' for i in range(len(documenti_esempio))])\n",
    "    axes[1,0].legend()\n",
    "\n",
    "# Distribuzione punteggi IDF\n",
    "idf_values = list(tfidf.idf_scores.values())\n",
    "axes[1,1].hist(idf_values, bins=10, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1,1].set_title('Distribuzione Punteggi IDF')\n",
    "axes[1,1].set_xlabel('Punteggio IDF')\n",
    "axes[1,1].set_ylabel('Frequenza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confronto similarità\n",
    "print(\"\\n🔍 CONFRONTO SIMILARITÀ:\")\n",
    "sim_bow = cosine_similarity(matrice_bow)\n",
    "sim_tfidf = cosine_similarity(matrice_tfidf)\n",
    "\n",
    "print(\"\\nSimilarità Doc 1 vs Doc 4 (entrambi parlano di gatti):\")\n",
    "print(f\"   BoW: {sim_bow[0][3]:.3f}\")\n",
    "print(f\"   TF-IDF: {sim_tfidf[0][3]:.3f}\")\n",
    "\n",
    "print(\"\\nSimilarità Doc 2 vs Doc 5 (entrambi parlano di cani nel parco):\")\n",
    "print(f\"   BoW: {sim_bow[1][4]:.3f}\")\n",
    "print(f\"   TF-IDF: {sim_tfidf[1][4]:.3f}\")\n",
    "\n",
    "print(\"\\n💡 VANTAGGI TF-IDF:\")\n",
    "print(\"✅ Riduce l'importanza di parole comuni\")\n",
    "print(\"✅ Evidenzia parole distintive\")\n",
    "print(\"✅ Migliore per ricerca e classificazione\")\n",
    "print(\"✅ Normalizza per lunghezza documento\")\n",
    "\n",
    "print(\"\\n❌ LIMITI TF-IDF:\")\n",
    "print(\"❌ Ancora ignora l'ordine delle parole\")\n",
    "print(\"❌ Non cattura sinonimi\")\n",
    "print(\"❌ Vettori ancora sparsi\")\n",
    "print(\"❌ Non comprende il contesto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Word2Vec - Embeddings Intelligenti\n",
    "\n",
    "**Word2Vec** rappresenta un salto qualitativo: impara rappresentazioni dense che catturano relazioni semantiche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarichiamo un modello Word2Vec pre-addestrato\n",
    "print(\"🧠 WORD2VEC - EMBEDDINGS INTELLIGENTI\\n\")\n",
    "\n",
    "# Per questo esempio, creeremo un piccolo corpus e addestriamo Word2Vec\n",
    "# In pratica, usereste modelli pre-addestrati su grandi corpus\n",
    "\n",
    "# Corpus più grande per addestrare Word2Vec\n",
    "corpus_esteso = [\n",
    "    \"il gatto dorme sul divano comodo\",\n",
    "    \"il cane corre veloce nel parco verde\",\n",
    "    \"i gatti amano dormire al sole caldo\",\n",
    "    \"i cani giocano insieme nel parco\",\n",
    "    \"il divano è molto comodo per dormire\",\n",
    "    \"nel parco ci sono molti alberi verdi\",\n",
    "    \"gli animali domestici sono fedeli compagni\",\n",
    "    \"il sole splende caldo in giardino\",\n",
    "    \"dormire è importante per la salute\",\n",
    "    \"correre fa bene alla salute fisica\",\n",
    "    \"i compagni fedeli sono preziosi\",\n",
    "    \"il giardino verde è pieno di fiori\",\n",
    "    \"gli alberi danno ombra fresca\",\n",
    "    \"i fiori colorati profumano dolcemente\",\n",
    "    \"la salute fisica è molto importante\",\n",
    "    \"gli animali selvatici vivono liberi\",\n",
    "    \"la natura è bella e selvaggia\",\n",
    "    \"i colori della natura sono vivaci\",\n",
    "    \"l'ombra fresca protegge dal caldo\",\n",
    "    \"i profumi dolci attirano le api\"\n",
    "]\n",
    "\n",
    "print(f\"📚 CORPUS ESTESO: {len(corpus_esteso)} frasi\")\n",
    "\n",
    "# Prepariamo i dati per Word2Vec\n",
    "frasi_tokenizzate = []\n",
    "for frase in corpus_esteso:\n",
    "    tokens = frase.lower().split()\n",
    "    frasi_tokenizzate.append(tokens)\n",
    "\n",
    "print(f\"🔤 FRASI TOKENIZZATE: {len(frasi_tokenizzate)} frasi\")\n",
    "print(\"Esempio:\", frasi_tokenizzate[0])\n",
    "\n",
    "# Addestriamo Word2Vec\n",
    "print(\"\\n🏋️ ADDESTRAMENTO WORD2VEC...\")\n",
    "model_w2v = Word2Vec(\n",
    "    sentences=frasi_tokenizzate,\n",
    "    vector_size=50,      # Dimensione degli embeddings\n",
    "    window=5,            # Finestra di contesto\n",
    "    min_count=1,         # Frequenza minima parole\n",
    "    workers=4,           # Thread paralleli\n",
    "    sg=0,                # 0=CBOW, 1=Skip-gram\n",
    "    epochs=100           # Numero di epoche\n",
    ")\n",
    "\n",
    "print(\"✅ Addestramento completato!\")\n",
    "print(f\"📊 Vocabolario: {len(model_w2v.wv.key_to_index)} parole\")\n",
    "print(f\"🔢 Dimensione embeddings: {model_w2v.wv.vector_size}\")\n",
    "\n",
    "# Esploriamo gli embeddings\n",
    "print(\"\\n🔍 ESPLORAZIONE EMBEDDINGS:\")\n",
    "\n",
    "# Parole nel vocabolario\n",
    "vocabolario_w2v = list(model_w2v.wv.key_to_index.keys())\n",
    "print(f\"Parole nel vocabolario: {vocabolario_w2v[:10]}...\")\n",
    "\n",
    "# Embedding di una parola specifica\n",
    "parola_test = 'gatto'\n",
    "if parola_test in model_w2v.wv:\n",
    "    embedding_gatto = model_w2v.wv[parola_test]\n",
    "    print(f\"\\n🐱 Embedding di '{parola_test}':\")\n",
    "    print(f\"   Dimensioni: {embedding_gatto.shape}\")\n",
    "    print(f\"   Primi 10 valori: {embedding_gatto[:10]}\")\n",
    "\n",
    "# Parole più simili\n",
    "print(f\"\\n🔗 PAROLE PIÙ SIMILI A '{parola_test}':\")\n",
    "try:\n",
    "    simili = model_w2v.wv.most_similar(parola_test, topn=5)\n",
    "    for parola, similarita in simili:\n",
    "        print(f\"   '{parola}': {similarita:.3f}\")\n",
    "except KeyError:\n",
    "    print(f\"   '{parola_test}' non trovato nel vocabolario\")\n",
    "\n",
    "# Test analogie (se possibile con il nostro piccolo corpus)\n",
    "print(\"\\n🧮 TEST ANALOGIE:\")\n",
    "try:\n",
    "    # gatto : dorme = cane : ?\n",
    "    analogia = model_w2v.wv.most_similar(\n",
    "        positive=['cane', 'dorme'], \n",
    "        negative=['gatto'], \n",
    "        topn=3\n",
    "    )\n",
    "    print(\"gatto : dorme = cane : ?\")\n",
    "    for parola, score in analogia:\n",
    "        print(f\"   {parola} ({score:.3f})\")\n",
    "except:\n",
    "    print(\"   Analogie non disponibili con questo corpus piccolo\")\n",
    "\n",
    "# Visualizzazione degli embeddings\n",
    "print(\"\\n📊 VISUALIZZAZIONE EMBEDDINGS (PCA 2D):\")\n",
    "\n",
    "# Selezioniamo alcune parole interessanti\n",
    "parole_interesse = ['gatto', 'cane', 'divano', 'parco', 'dormire', 'correre', \n",
    "                   'sole', 'alberi', 'salute', 'natura']\n",
    "parole_disponibili = [p for p in parole_interesse if p in model_w2v.wv]\n",
    "\n",
    "if len(parole_disponibili) > 3:\n",
    "    # Ottieni embeddings\n",
    "    embeddings_viz = np.array([model_w2v.wv[parola] for parola in parole_disponibili])\n",
    "    \n",
    "    # Riduci dimensionalità con PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_viz)\n",
    "    \n",
    "    # Visualizza\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         s=100, alpha=0.7, c=range(len(parole_disponibili)), \n",
    "                         cmap='tab10')\n",
    "    \n",
    "    for i, parola in enumerate(parole_disponibili):\n",
    "        ax1.annotate(parola, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} varianza)')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} varianza)')\n",
    "    ax1.set_title('Word2Vec Embeddings (PCA 2D)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Matrice di similarità\n",
    "    n_parole = len(parole_disponibili)\n",
    "    matrice_sim = np.zeros((n_parole, n_parole))\n",
    "    \n",
    "    for i, parola1 in enumerate(parole_disponibili):\n",
    "        for j, parola2 in enumerate(parole_disponibili):\n",
    "            if i != j:\n",
    "                sim = model_w2v.wv.similarity(parola1, parola2)\n",
    "                matrice_sim[i, j] = sim\n",
    "            else:\n",
    "                matrice_sim[i, j] = 1.0\n",
    "    \n",
    "    sns.heatmap(matrice_sim, \n",
    "                xticklabels=parole_disponibili,\n",
    "                yticklabels=parole_disponibili,\n",
    "                annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                center=0, ax=ax2)\n",
    "    ax2.set_title('Matrice Similarità Word2Vec')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"👀 OSSERVAZIONI:\")\n",
    "    print(\"   • Parole semanticamente simili sono vicine nello spazio\")\n",
    "    print(\"   • Gli embeddings catturano relazioni non evidenti in BoW/TF-IDF\")\n",
    "    print(\"   • La dimensionalità è molto più bassa ma più informativa\")\n",
    "\n",
    "print(\"\\n💡 VANTAGGI WORD2VEC:\")\n",
    "print(\"✅ Cattura relazioni semantiche\")\n",
    "print(\"✅ Embeddings densi (pochi zeri)\")\n",
    "print(\"✅ Dimensionalità controllabile\")\n",
    "print(\"✅ Supporta analogie\")\n",
    "print(\"✅ Transfer learning possibile\")\n",
    "\n",
    "print(\"\\n❌ LIMITI WORD2VEC:\")\n",
    "print(\"❌ Una rappresentazione per parola (no polisemia)\")\n",
    "print(\"❌ Non gestisce parole fuori vocabolario\")\n",
    "print(\"❌ Richiede grandi corpus per qualità\")\n",
    "print(\"❌ Non considera contesto della frase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Confronto Completo dei Metodi\n",
    "\n",
    "Confrontiamo tutti i metodi su un task pratico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto completo dei metodi\n",
    "print(\"🔍 CONFRONTO COMPLETO: BoW vs TF-IDF vs Word2Vec\\n\")\n",
    "\n",
    "# Task: Trovare documenti simili\n",
    "query = \"gatto che dorme\"\n",
    "print(f\"🎯 QUERY: '{query}'\")\n",
    "print(f\"📚 DOCUMENTI DA CERCARE:\")\n",
    "for i, doc in enumerate(documenti_esempio, 1):\n",
    "    print(f\"   {i}. {doc}\")\n",
    "\n",
    "# 1. BoW\n",
    "print(\"\\n1️⃣ RICERCA CON BAG OF WORDS:\")\n",
    "query_bow = bow.documento_to_vettore(query)\n",
    "sim_bow_query = cosine_similarity([query_bow], matrice_bow)[0]\n",
    "\n",
    "risultati_bow = [(i+1, sim, doc) for i, (sim, doc) in enumerate(zip(sim_bow_query, documenti_esempio))]\n",
    "risultati_bow.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (doc_id, similarita, doc) in enumerate(risultati_bow[:3], 1):\n",
    "    print(f\"   {rank}. Doc {doc_id} (sim: {similarita:.3f}): {doc}\")\n",
    "\n",
    "# 2. TF-IDF\n",
    "print(\"\\n2️⃣ RICERCA CON TF-IDF:\")\n",
    "query_tfidf = tfidf.documento_to_tfidf(query)\n",
    "sim_tfidf_query = cosine_similarity([query_tfidf], matrice_tfidf)[0]\n",
    "\n",
    "risultati_tfidf = [(i+1, sim, doc) for i, (sim, doc) in enumerate(zip(sim_tfidf_query, documenti_esempio))]\n",
    "risultati_tfidf.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (doc_id, similarita, doc) in enumerate(risultati_tfidf[:3], 1):\n",
    "    print(f\"   {rank}. Doc {doc_id} (sim: {similarita:.3f}): {doc}\")\n",
    "\n",
    "# 3. Word2Vec (media degli embeddings)\n",
    "print(\"\\n3️⃣ RICERCA CON WORD2VEC:\")\n",
    "\n",
    "def documento_to_w2v(documento, model):\n",
    "    \"\"\"Converte documento in embedding medio\"\"\"\n",
    "    parole = documento.lower().split()\n",
    "    embeddings = []\n",
    "    \n",
    "    for parola in parole:\n",
    "        if parola in model.wv:\n",
    "            embeddings.append(model.wv[parola])\n",
    "    \n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Converti query e documenti\n",
    "query_w2v = documento_to_w2v(query, model_w2v)\n",
    "docs_w2v = [documento_to_w2v(doc, model_w2v) for doc in documenti_esempio]\n",
    "\n",
    "# Calcola similarità\n",
    "sim_w2v_query = cosine_similarity([query_w2v], docs_w2v)[0]\n",
    "\n",
    "risultati_w2v = [(i+1, sim, doc) for i, (sim, doc) in enumerate(zip(sim_w2v_query, documenti_esempio))]\n",
    "risultati_w2v.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (doc_id, similarita, doc) in enumerate(risultati_w2v[:3], 1):\n",
    "    print(f\"   {rank}. Doc {doc_id} (sim: {similarita:.3f}): {doc}\")\n",
    "\n",
    "# Visualizzazione comparativa\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metodi = ['BoW', 'TF-IDF', 'Word2Vec']\n",
    "risultati_tutti = [risultati_bow, risultati_tfidf, risultati_w2v]\n",
    "\n",
    "for i, (metodo, risultati) in enumerate(zip(metodi, risultati_tutti)):\n",
    "    doc_ids = [r[0] for r in risultati]\n",
    "    similarita = [r[1] for r in risultati]\n",
    "    \n",
    "    bars = axes[i].bar(doc_ids, similarita, alpha=0.7, \n",
    "                      color=['gold' if j == 0 else 'lightblue' for j in range(len(doc_ids))])\n",
    "    \n",
    "    axes[i].set_title(f'Similarità con Query - {metodo}')\n",
    "    axes[i].set_xlabel('Documento ID')\n",
    "    axes[i].set_ylabel('Similarità Coseno')\n",
    "    axes[i].set_ylim(0, max(similarita) * 1.1 if max(similarita) > 0 else 1)\n",
    "    \n",
    "    # Evidenzia il migliore\n",
    "    best_idx = np.argmax(similarita)\n",
    "    bars[best_idx].set_color('gold')\n",
    "    \n",
    "    # Aggiungi valori sopra le barre\n",
    "    for bar, sim in zip(bars, similarita):\n",
    "        if sim > 0:\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{sim:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabella riassuntiva\n",
    "print(\"\\n📊 TABELLA RIASSUNTIVA:\")\n",
    "df_confronto = pd.DataFrame({\n",
    "    'Metodo': ['BoW', 'TF-IDF', 'Word2Vec'],\n",
    "    'Miglior Match': [\n",
    "        f\"Doc {risultati_bow[0][0]} ({risultati_bow[0][1]:.3f})\",\n",
    "        f\"Doc {risultati_tfidf[0][0]} ({risultati_tfidf[0][1]:.3f})\",\n",
    "        f\"Doc {risultati_w2v[0][0]} ({risultati_w2v[0][1]:.3f})\"\n",
    "    ],\n",
    "    'Documento': [\n",
    "        risultati_bow[0][2],\n",
    "        risultati_tfidf[0][2],\n",
    "        risultati_w2v[0][2]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(df_confronto.to_string(index=False))\n",
    "\n",
    "print(\"\\n💡 ANALISI RISULTATI:\")\n",
    "print(\"• BoW: Trova corrispondenze esatte di parole\")\n",
    "print(\"• TF-IDF: Bilancia frequenza e rarità\")\n",
    "print(\"• Word2Vec: Cattura similarità semantica\")\n",
    "print(\"• Ogni metodo ha i suoi punti di forza\")\n",
    "\n",
    "print(\"\\n🎯 QUANDO USARE COSA:\")\n",
    "print(\"📊 BoW: Classificazione semplice, analisi frequenze\")\n",
    "print(\"📈 TF-IDF: Ricerca documenti, classificazione testi\")\n",
    "print(\"🧠 Word2Vec: Analisi semantica, raccomandazioni, AI avanzata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎨 Visualizzazione Avanzata con t-SNE\n",
    "\n",
    "Usiamo **t-SNE** per visualizzare gli embeddings in modo più intuitivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione avanzata con t-SNE\n",
    "print(\"🎨 VISUALIZZAZIONE AVANZATA CON t-SNE\\n\")\n",
    "\n",
    "# Prepariamo i dati per t-SNE\n",
    "if len(vocabolario_w2v) > 10:  # Assicuriamoci di avere abbastanza parole\n",
    "    \n",
    "    # Selezioniamo parole interessanti per la visualizzazione\n",
    "    parole_viz = []\n",
    "    categorie = {\n",
    "        'Animali': ['gatto', 'cane', 'animali'],\n",
    "        'Luoghi': ['divano', 'parco', 'giardino', 'casa'],\n",
    "        'Azioni': ['dormire', 'correre', 'giocare'],\n",
    "        'Natura': ['sole', 'alberi', 'fiori', 'natura'],\n",
    "        'Qualità': ['comodo', 'verde', 'caldo', 'bello']\n",
    "    }\n",
    "    \n",
    "    parole_selezionate = []\n",
    "    etichette_categoria = []\n",
    "    \n",
    "    for categoria, parole in categorie.items():\n",
    "        for parola in parole:\n",
    "            if parola in model_w2v.wv:\n",
    "                parole_selezionate.append(parola)\n",
    "                etichette_categoria.append(categoria)\n",
    "    \n",
    "    if len(parole_selezionate) > 5:\n",
    "        print(f\"📊 Visualizzando {len(parole_selezionate)} parole in {len(set(etichette_categoria))} categorie\")\n",
    "        \n",
    "        # Ottieni embeddings\n",
    "        embeddings_tsne = np.array([model_w2v.wv[parola] for parola in parole_selezionate])\n",
    "        \n",
    "        # Applica t-SNE\n",
    "        print(\"🔄 Applicando t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(parole_selezionate)-1))\n",
    "        embeddings_tsne_2d = tsne.fit_transform(embeddings_tsne)\n",
    "        \n",
    "        # Crea DataFrame per visualizzazione\n",
    "        df_viz = pd.DataFrame({\n",
    "            'x': embeddings_tsne_2d[:, 0],\n",
    "            'y': embeddings_tsne_2d[:, 1],\n",
    "            'parola': parole_selezionate,\n",
    "            'categoria': etichette_categoria\n",
    "        })\n",
    "        \n",
    "        # Visualizzazione con matplotlib\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Plot 1: Colorato per categoria\n",
    "        categorie_uniche = list(set(etichette_categoria))\n",
    "        colori = plt.cm.Set3(np.linspace(0, 1, len(categorie_uniche)))\n",
    "        \n",
    "        for i, categoria in enumerate(categorie_uniche):\n",
    "            mask = df_viz['categoria'] == categoria\n",
    "            ax1.scatter(df_viz[mask]['x'], df_viz[mask]['y'], \n",
    "                       c=[colori[i]], label=categoria, s=100, alpha=0.7)\n",
    "            \n",
    "            # Aggiungi etichette\n",
    "            for _, row in df_viz[mask].iterrows():\n",
    "                ax1.annotate(row['parola'], (row['x'], row['y']),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax1.set_title('t-SNE Word2Vec Embeddings (per Categoria)')\n",
    "        ax1.set_xlabel('t-SNE Dimensione 1')\n",
    "        ax1.set_ylabel('t-SNE Dimensione 2')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Heatmap distanze\n",
    "        n_parole_viz = len(parole_selezionate)\n",
    "        matrice_distanze = np.zeros((n_parole_viz, n_parole_viz))\n",
    "        \n",
    "        for i in range(n_parole_viz):\n",
    "            for j in range(n_parole_viz):\n",
    "                if i != j:\n",
    "                    # Distanza euclidea nello spazio t-SNE\n",
    "                    dist = np.linalg.norm(embeddings_tsne_2d[i] - embeddings_tsne_2d[j])\n",
    "                    matrice_distanze[i, j] = dist\n",
    "        \n",
    "        sns.heatmap(matrice_distanze,\n",
    "                   xticklabels=parole_selezionate,\n",
    "                   yticklabels=parole_selezionate,\n",
    "                   annot=True, fmt='.1f', cmap='viridis_r',\n",
    "                   ax=ax2)\n",
    "        ax2.set_title('Matrice Distanze t-SNE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analisi dei cluster\n",
    "        print(\"\\n🔍 ANALISI CLUSTER:\")\n",
    "        for categoria in categorie_uniche:\n",
    "            parole_cat = df_viz[df_viz['categoria'] == categoria]['parola'].tolist()\n",
    "            if len(parole_cat) > 1:\n",
    "                print(f\"\\n📂 {categoria}:\")\n",
    "                for parola in parole_cat:\n",
    "                    print(f\"   • {parola}\")\n",
    "                \n",
    "                # Calcola coesione del cluster\n",
    "                embeddings_cat = [model_w2v.wv[p] for p in parole_cat]\n",
    "                if len(embeddings_cat) > 1:\n",
    "                    sim_interna = []\n",
    "                    for i in range(len(embeddings_cat)):\n",
    "                        for j in range(i+1, len(embeddings_cat)):\n",
    "                            sim = cosine_similarity([embeddings_cat[i]], [embeddings_cat[j]])[0][0]\n",
    "                            sim_interna.append(sim)\n",
    "                    \n",
    "                    coesione = np.mean(sim_interna)\n",
    "                    print(f\"   Coesione cluster: {coesione:.3f}\")\n",
    "        \n",
    "        print(\"\\n💡 INTERPRETAZIONE t-SNE:\")\n",
    "        print(\"• Parole vicine hanno significati correlati\")\n",
    "        print(\"• I cluster riflettono categorie semantiche\")\n",
    "        print(\"• t-SNE preserva le vicinanze locali\")\n",
    "        print(\"• Utile per esplorare il vocabolario\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️ Poche parole disponibili per t-SNE\")\n",
    "else:\n",
    "    print(\"⚠️ Vocabolario troppo piccolo per visualizzazione avanzata\")\n",
    "\n",
    "print(\"\\n🎯 APPLICAZIONI PRATICHE t-SNE:\")\n",
    "print(\"🔍 Esplorazione vocabolario\")\n",
    "print(\"🏷️ Identificazione cluster semantici\")\n",
    "print(\"🐛 Debug di modelli di embeddings\")\n",
    "print(\"📊 Visualizzazione per presentazioni\")\n",
    "print(\"🔬 Ricerca in linguistica computazionale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Applicazione Pratica: Sistema di Raccomandazione\n",
    "\n",
    "Costruiamo un sistema di raccomandazione usando gli embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistema di raccomandazione con embeddings\n",
    "class SistemaRaccomandazione:\n",
    "    def __init__(self, metodo='word2vec'):\n",
    "        self.metodo = metodo\n",
    "        self.documenti = []\n",
    "        self.embeddings = []\n",
    "        self.titoli = []\n",
    "        \n",
    "    def aggiungi_documenti(self, documenti, titoli=None):\n",
    "        \"\"\"Aggiunge documenti al sistema\"\"\"\n",
    "        self.documenti = documenti\n",
    "        self.titoli = titoli if titoli else [f\"Doc {i+1}\" for i in range(len(documenti))]\n",
    "        \n",
    "        # Calcola embeddings in base al metodo\n",
    "        if self.metodo == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "            self.embeddings = vectorizer.fit_transform(documenti).toarray()\n",
    "            \n",
    "        elif self.metodo == 'word2vec':\n",
    "            self.embeddings = []\n",
    "            for doc in documenti:\n",
    "                embedding = documento_to_w2v(doc, model_w2v)\n",
    "                self.embeddings.append(embedding)\n",
    "            self.embeddings = np.array(self.embeddings)\n",
    "    \n",
    "    def raccomanda(self, query, n_raccomandazioni=3):\n",
    "        \"\"\"Raccomanda documenti simili alla query\"\"\"\n",
    "        if self.metodo == 'tfidf':\n",
    "            # Per TF-IDF dovremmo usare lo stesso vectorizer\n",
    "            # Semplificazione: usiamo Word2Vec\n",
    "            query_embedding = documento_to_w2v(query, model_w2v)\n",
    "        else:\n",
    "            query_embedding = documento_to_w2v(query, model_w2v)\n",
    "        \n",
    "        # Calcola similarità\n",
    "        similarita = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # Ordina per similarità\n",
    "        indici_ordinati = np.argsort(similarita)[::-1]\n",
    "        \n",
    "        raccomandazioni = []\n",
    "        for i in range(min(n_raccomandazioni, len(indici_ordinati))):\n",
    "            idx = indici_ordinati[i]\n",
    "            raccomandazioni.append({\n",
    "                'titolo': self.titoli[idx],\n",
    "                'documento': self.documenti[idx],\n",
    "                'similarita': similarita[idx],\n",
    "                'rank': i + 1\n",
    "            })\n",
    "        \n",
    "        return raccomandazioni\n",
    "\n",
    "# Test del sistema di raccomandazione\n",
    "print(\"🛠️ SISTEMA DI RACCOMANDAZIONE CON EMBEDDINGS\\n\")\n",
    "\n",
    "# Dataset di esempio (articoli di blog)\n",
    "articoli = [\n",
    "    \"Come addestrare il tuo gatto a usare la lettiera. Consigli pratici per proprietari di gatti.\",\n",
    "    \"I migliori parchi per cani nella tua città. Dove portare il tuo cane a giocare.\",\n",
    "    \"Ricette salutari per una dieta equilibrata. Mangiare bene per stare in forma.\",\n",
    "    \"Esercizi di yoga per rilassarsi dopo una giornata stressante. Trova la pace interiore.\",\n",
    "    \"Giardinaggio urbano: come coltivare piante in appartamento. Verde in casa.\",\n",
    "    \"Tecniche di meditazione per principianti. Inizia il tuo percorso di mindfulness.\",\n",
    "    \"Cura degli animali domestici: consigli veterinari per gatti e cani.\",\n",
    "    \"Alimentazione naturale per animali: cosa dare da mangiare ai tuoi pets.\",\n",
    "    \"Creare un giardino zen in casa. Spazi di tranquillità domestici.\",\n",
    "    \"Sport e benessere: l'importanza dell'attività fisica per la salute mentale.\"\n",
    "]\n",
    "\n",
    "titoli_articoli = [\n",
    "    \"Addestramento Gatti\",\n",
    "    \"Parchi per Cani\", \n",
    "    \"Ricette Salutari\",\n",
    "    \"Yoga e Relax\",\n",
    "    \"Giardinaggio Urbano\",\n",
    "    \"Meditazione Base\",\n",
    "    \"Cura Animali\",\n",
    "    \"Alimentazione Pets\",\n",
    "    \"Giardino Zen\",\n",
    "    \"Sport e Benessere\"\n",
    "]\n",
    "\n",
    "print(\"📚 DATASET ARTICOLI:\")\n",
    "for i, (titolo, articolo) in enumerate(zip(titoli_articoli, articoli), 1):\n",
    "    print(f\"   {i}. {titolo}: {articolo[:50]}...\")\n",
    "\n",
    "# Inizializza sistema\n",
    "sistema = SistemaRaccomandazione(metodo='word2vec')\n",
    "sistema.aggiungi_documenti(articoli, titoli_articoli)\n",
    "\n",
    "# Test con diverse query\n",
    "query_test = [\n",
    "    \"Ho un gatto e voglio consigli\",\n",
    "    \"Cerco modi per rilassarmi\",\n",
    "    \"Voglio coltivare piante in casa\",\n",
    "    \"Consigli per animali domestici\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 TEST RACCOMANDAZIONI:\\n\")\n",
    "\n",
    "for query in query_test:\n",
    "    print(f\"🎯 QUERY: '{query}'\")\n",
    "    raccomandazioni = sistema.raccomanda(query, n_raccomandazioni=3)\n",
    "    \n",
    "    print(\"📋 RACCOMANDAZIONI:\")\n",
    "    for racc in raccomandazioni:\n",
    "        print(f\"   {racc['rank']}. {racc['titolo']} (sim: {racc['similarita']:.3f})\")\n",
    "        print(f\"      {racc['documento'][:60]}...\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Visualizzazione delle raccomandazioni\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, query in enumerate(query_test):\n",
    "    raccomandazioni = sistema.raccomanda(query, n_raccomandazioni=5)\n",
    "    \n",
    "    titoli_racc = [r['titolo'] for r in raccomandazioni]\n",
    "    similarita_racc = [r['similarita'] for r in raccomandazioni]\n",
    "    \n",
    "    bars = axes[i].barh(range(len(titoli_racc)), similarita_racc, \n",
    "                       color=['gold', 'silver', 'orange', 'lightblue', 'lightgreen'][:len(titoli_racc)])\n",
    "    \n",
    "    axes[i].set_yticks(range(len(titoli_racc)))\n",
    "    axes[i].set_yticklabels(titoli_racc)\n",
    "    axes[i].set_xlabel('Similarità')\n",
    "    axes[i].set_title(f'Query: \"{query[:20]}...\"')\n",
    "    \n",
    "    # Aggiungi valori\n",
    "    for bar, sim in zip(bars, similarita_racc):\n",
    "        axes[i].text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{sim:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 VANTAGGI SISTEMA CON EMBEDDINGS:\")\n",
    "print(\"✅ Cattura similarità semantica\")\n",
    "print(\"✅ Non richiede parole esatte\")\n",
    "print(\"✅ Gestisce sinonimi e concetti correlati\")\n",
    "print(\"✅ Scalabile a grandi dataset\")\n",
    "print(\"✅ Personalizzabile per domini specifici\")\n",
    "\n",
    "print(\"\\n🚀 POSSIBILI MIGLIORAMENTI:\")\n",
    "print(\"🔧 Embeddings pre-addestrati più grandi\")\n",
    "print(\"🔧 Combinazione di più metodi\")\n",
    "print(\"🔧 Feedback degli utenti\")\n",
    "print(\"🔧 Filtri per categoria/data\")\n",
    "print(\"🔧 Embeddings contestuali (BERT, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Esercizio Pratico: Il Tuo Sistema di Embeddings\n",
    "\n",
    "Ora tocca a te! Crea un sistema personalizzato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESERCIZIO: Crea il tuo sistema di embeddings\n",
    "print(\"🎯 ESERCIZIO: Costruisci il Tuo Sistema di Embeddings\\n\")\n",
    "\n",
    "class MioSistemaEmbeddings:\n",
    "    def __init__(self, metodo='combinato'):\n",
    "        self.metodo = metodo\n",
    "        self.vocabolario = {}\n",
    "        self.embeddings_parole = {}\n",
    "        \n",
    "    def addestra(self, documenti):\n",
    "        \"\"\"TODO: Implementa il tuo metodo di addestramento\"\"\"\n",
    "        print(f\"🏋️ Addestrando con metodo: {self.metodo}\")\n",
    "        \n",
    "        # TODO: Implementa qui la tua logica\n",
    "        # Suggerimenti:\n",
    "        # 1. Combina TF-IDF + Word2Vec\n",
    "        # 2. Usa pesi diversi per parole diverse\n",
    "        # 3. Aggiungi informazioni di posizione\n",
    "        # 4. Considera la lunghezza del documento\n",
    "        \n",
    "        # Esempio base:\n",
    "        if self.metodo == 'combinato':\n",
    "            # Combina TF-IDF e Word2Vec\n",
    "            pass\n",
    "        \n",
    "        print(\"✅ Addestramento completato!\")\n",
    "    \n",
    "    def documento_to_embedding(self, documento):\n",
    "        \"\"\"TODO: Converte documento in embedding\"\"\"\n",
    "        # TODO: Implementa la tua logica di conversione\n",
    "        \n",
    "        # Esempio semplice: usa Word2Vec\n",
    "        return documento_to_w2v(documento, model_w2v)\n",
    "    \n",
    "    def trova_simili(self, query, documenti, n=3):\n",
    "        \"\"\"TODO: Trova documenti simili\"\"\"\n",
    "        query_emb = self.documento_to_embedding(query)\n",
    "        \n",
    "        similarita = []\n",
    "        for doc in documenti:\n",
    "            doc_emb = self.documento_to_embedding(doc)\n",
    "            sim = cosine_similarity([query_emb], [doc_emb])[0][0]\n",
    "            similarita.append(sim)\n",
    "        \n",
    "        # Ordina e restituisci top N\n",
    "        indici = np.argsort(similarita)[::-1][:n]\n",
    "        risultati = [(documenti[i], similarita[i]) for i in indici]\n",
    "        \n",
    "        return risultati\n",
    "\n",
    "# Test del tuo sistema\n",
    "print(\"🧪 TEST DEL TUO SISTEMA:\\n\")\n",
    "\n",
    "# Dati di test\n",
    "docs_test = [\n",
    "    \"Il machine learning è una branca dell'intelligenza artificiale\",\n",
    "    \"I gatti sono animali domestici molto indipendenti\",\n",
    "    \"La programmazione Python è utile per data science\",\n",
    "    \"Gli algoritmi di deep learning usano reti neurali\",\n",
    "    \"I cani sono compagni fedeli e giocherelloni\"\n",
    "]\n",
    "\n",
    "mio_sistema = MioSistemaEmbeddings(metodo='combinato')\n",
    "mio_sistema.addestra(docs_test)\n",
    "\n",
    "# Test query\n",
    "query_test = \"Voglio imparare l'intelligenza artificiale\"\n",
    "print(f\"🔍 Query: '{query_test}'\")\n",
    "\n",
    "risultati = mio_sistema.trova_simili(query_test, docs_test, n=3)\n",
    "\n",
    "print(\"\\n📋 Risultati:\")\n",
    "for i, (doc, sim) in enumerate(risultati, 1):\n",
    "    print(f\"   {i}. (sim: {sim:.3f}) {doc}\")\n",
    "\n",
    "print(\"\\n💪 SFIDE PER TE:\")\n",
    "sfide = [\n",
    "    \"🔧 Implementa un metodo che combina TF-IDF e Word2Vec\",\n",
    "    \"⚖️ Aggiungi pesi diversi per parole importanti\",\n",
    "    \"📍 Considera la posizione delle parole nel documento\",\n",
    "    \"📏 Normalizza per la lunghezza del documento\",\n",
    "    \"🎯 Aggiungi filtri per categoria o data\",\n",
    "    \"📊 Implementa metriche di valutazione\",\n",
    "    \"🔄 Crea un sistema di feedback per migliorare\",\n",
    "    \"🌐 Gestisci documenti multilingue\"\n",
    "]\n",
    "\n",
    "for sfida in sfide:\n",
    "    print(f\"   {sfida}\")\n",
    "\n",
    "print(\"\\n🎓 SUGGERIMENTI:\")\n",
    "suggerimenti = [\n",
    "    \"📚 Studia embeddings pre-addestrati (GloVe, FastText)\",\n",
    "    \"🔬 Sperimenta con diversi metodi di aggregazione\",\n",
    "    \"📈 Misura sempre le prestazioni su dati reali\",\n",
    "    \"🔍 Analizza i casi in cui il sistema fallisce\",\n",
    "    \"💡 Considera il contesto del tuo dominio applicativo\"\n",
    "]\n",
    "\n",
    "for suggerimento in suggerimenti:\n",
    "    print(f\"   {suggerimento}\")\n",
    "\n",
    "# Spazio per il tuo codice\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✏️ SPAZIO PER IL TUO CODICE:\")\n",
    "print(\"   Modifica la classe MioSistemaEmbeddings sopra!\")\n",
    "print(\"   Testa con i tuoi dati!\")\n",
    "print(\"   Confronta con i metodi standard!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Cosa Abbiamo Imparato\n",
    "\n",
    "Congratulazioni! Hai completato il notebook sui Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riassunto finale\n",
    "print(\"🎉 CONGRATULAZIONI! Hai completato il Notebook 3\\n\")\n",
    "\n",
    "concetti_appresi = [\n",
    "    \"✅ Perché l'AI ha bisogno di rappresentazioni numeriche\",\n",
    "    \"✅ Implementazione Bag of Words da zero\",\n",
    "    \"✅ Calcolo TF-IDF e sua interpretazione\",\n",
    "    \"✅ Concetti di Word2Vec e embeddings densi\",\n",
    "    \"✅ Visualizzazione embeddings con PCA e t-SNE\",\n",
    "    \"✅ Confronto sistematico dei metodi\",\n",
    "    \"✅ Applicazione pratica: sistema di raccomandazione\",\n",
    "    \"✅ Metriche di similarità (coseno)\",\n",
    "    \"✅ Analisi di cluster semantici\"\n",
    "]\n",
    "\n",
    "print(\"📚 CONCETTI APPRESI:\")\n",
    "for concetto in concetti_appresi:\n",
    "    print(f\"   {concetto}\")\n",
    "\n",
    "print(\"\\n🛠️ COMPETENZE PRATICHE:\")\n",
    "competenze = [\n",
    "    \"🔢 Implementazione algoritmi di embedding\",\n",
    "    \"📊 Visualizzazione spazi vettoriali\",\n",
    "    \"🔍 Calcolo similarità semantica\",\n",
    "    \"⚖️ Confronto metodi diversi\",\n",
    "    \"🎯 Costruzione sistemi di raccomandazione\",\n",
    "    \"📈 Valutazione qualità embeddings\"\n",
    "]\n",
    "\n",
    "for competenza in competenze:\n",
    "    print(f\"   {competenza}\")\n",
    "\n",
    "# Tabella riassuntiva metodi\n",
    "print(\"\\n📊 TABELLA RIASSUNTIVA METODI:\")\n",
    "df_riassunto = pd.DataFrame({\n",
    "    'Metodo': ['Bag of Words', 'TF-IDF', 'Word2Vec'],\n",
    "    'Tipo': ['Sparso', 'Sparso', 'Denso'],\n",
    "    'Dimensionalità': ['Alta (= vocabolario)', 'Alta (= vocabolario)', 'Bassa (50-300)'],\n",
    "    'Semantica': ['No', 'Parziale', 'Sì'],\n",
    "    'Velocità': ['Veloce', 'Veloce', 'Media'],\n",
    "    'Uso Principale': ['Classificazione base', 'Ricerca documenti', 'AI avanzata']\n",
    "})\n",
    "\n",
    "print(df_riassunto.to_string(index=False))\n",
    "\n",
    "print(\"\\n🚀 PROSSIMI PASSI:\")\n",
    "print(\"   📖 Notebook 4: Reti Neurali Ricorrenti\")\n",
    "print(\"   📖 Notebook 5: Transformer e LLM\")\n",
    "print(\"   📖 Notebook 6: Vector Stores e RAG\")\n",
    "\n",
    "print(\"\\n💡 SUGGERIMENTI PER CONTINUARE:\")\n",
    "print(\"   • Sperimenta con embeddings pre-addestrati\")\n",
    "print(\"   • Testa su dataset del tuo dominio\")\n",
    "print(\"   • Confronta con embeddings contestuali (BERT)\")\n",
    "print(\"   • Costruisci applicazioni pratiche\")\n",
    "\n",
    "print(\"\\n🌟 RICORDA:\")\n",
    "print(\"   Gli embeddings sono la base di tutta l'AI moderna.\")\n",
    "print(\"   La qualità degli embeddings determina il successo del modello!\")\n",
    "\n",
    "# Badge di completamento\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "circle = plt.Circle((0.5, 0.5), 0.4, color='lightgreen', alpha=0.8)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "ax.text(0.5, 0.6, '🔢', ha='center', va='center', fontsize=40)\n",
    "ax.text(0.5, 0.45, 'COMPLETATO', ha='center', va='center', \n",
    "        fontsize=14, fontweight='bold')\n",
    "ax.text(0.5, 0.35, 'Notebook 3', ha='center', va='center', \n",
    "        fontsize=12)\n",
    "ax.text(0.5, 0.25, 'Word Embeddings', ha='center', va='center', \n",
    "        fontsize=10, style='italic')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Badge di Completamento', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🔬 APPROFONDIMENTI AVANZATI (opzionali):\")\n",
    "print(\"1. Embeddings contestuali (ELMo, BERT)\")\n",
    "print(\"2. FastText per parole fuori vocabolario\")\n",
    "print(\"3. Sentence embeddings (Doc2Vec, Universal Sentence Encoder)\")\n",
    "print(\"4. Embeddings multimodali (testo + immagini)\")\n",
    "print(\"5. Fine-tuning embeddings per domini specifici\")\n",
    "\n",
    "print(\"\\n🎯 PROGETTI PRATICI SUGGERITI:\")\n",
    "print(\"• Sistema di ricerca semantica\")\n",
    "print(\"• Classificatore di sentimenti\")\n",
    "print(\"• Chatbot con comprensione semantica\")\n",
    "print(\"• Analizzatore di similarità documenti\")\n",
    "print(\"• Sistema di tag automatici\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Note e Riflessioni\n",
    "\n",
    "Usa questa sezione per annotare le tue riflessioni:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le mie note sui Word Embeddings:**\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "**Domande per approfondire:**\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "**Idee per applicazioni:**\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "\n",
    "**Esperimenti da provare:**\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
