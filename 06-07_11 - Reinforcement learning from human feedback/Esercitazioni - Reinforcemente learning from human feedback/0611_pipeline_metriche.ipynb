{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 4 (06/11) ‚Äî Pipeline Completa e Metriche Automatiche\n",
    "\n",
    "**Obiettivo pratico (4 ore di codice)**: Integrare pre-training ‚Üí SFT ‚Üí RLHF in una pipeline completa, confrontare modelli, e implementare metriche automatiche (BLEU, ROUGE).\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è Roadmap della lezione (240 minuti di codice)\n",
    "\n",
    "| **Sezione** | **Contenuto** | **Tempo stimato** |\n",
    "|-------------|---------------|-------------------|\n",
    "| 1 | Setup e teoria pipeline RLHF | 20' |\n",
    "| 2 | Integrazione pipeline completa | 40' |\n",
    "| 3 | Casi d'uso RLHF reali | 30' |\n",
    "| 4 | RLHF simulato su dataset preferenze | 40' |\n",
    "| 5 | Confronto base vs SFT vs RLHF | 40' |\n",
    "| 6 | Metriche automatiche (BLEU, ROUGE) | 50' |\n",
    "| 7 | Discussione: valutazione automatica vs umana | 20' |\n",
    "| **TOTALE** | | **240'** |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Leggi dopo (teoria fuori orario)\n",
    "\n",
    "### Pipeline RLHF completa\n",
    "```\n",
    "1. Pre-training\n",
    "   - Dataset: centinaia miliardi token (web, libri, code)\n",
    "   - Obiettivo: next token prediction\n",
    "   - Output: modello base (es. GPT-3, LLaMA)\n",
    "   ‚Üì\n",
    "2. Supervised Fine-Tuning (SFT)\n",
    "   - Dataset: migliaia esempi labeled (instruction-following)\n",
    "   - Obiettivo: imparare a seguire istruzioni\n",
    "   - Output: modello SFT (es. GPT-3.5)\n",
    "   ‚Üì\n",
    "3. Reward Model Training\n",
    "   - Dataset: decine migliaia coppie preferenze\n",
    "   - Obiettivo: predire preferenze umane\n",
    "   - Output: reward model\n",
    "   ‚Üì\n",
    "4. RL Fine-tuning (PPO/DPO)\n",
    "   - Dataset: prompt + reward model\n",
    "   - Obiettivo: massimizzare reward\n",
    "   - Output: modello allineato (es. ChatGPT)\n",
    "```\n",
    "\n",
    "### Esempio: OpenAI ChatGPT\n",
    "- **Pre-training**: GPT-3 (175B parametri, 300B token)\n",
    "- **SFT**: ~13K esempi instruction-following\n",
    "- **Reward Model**: ~33K coppie preferenze\n",
    "- **PPO**: migliaia iterazioni\n",
    "- **Costo totale stimato**: $10-20M\n",
    "\n",
    "### Sfide pratiche\n",
    "1. **Dataset costosi**: annotazione umana $1-10 per esempio\n",
    "2. **Annotatori in disaccordo**: inter-annotator agreement ~70-80%\n",
    "3. **Allineamento imperfetto**: modello pu√≤ comunque allucinare\n",
    "4. **Generalizzazione**: performance su distribuzione diversa pu√≤ degradare\n",
    "5. **Test continui**: necessit√† di monitoraggio post-deployment\n",
    "\n",
    "### Metriche automatiche\n",
    "- **BLEU**: precision n-grammi (traduzione)\n",
    "- **ROUGE**: recall n-grammi (summarization)\n",
    "- **Perplessit√†**: quanto il modello √® \"sorpreso\" dal testo\n",
    "- **Limiti**: non catturano semantica, coerenza, utilit√†\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e teoria pipeline RLHF (20 minuti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione librerie\n",
    "!pip install torch transformers datasets nltk rouge-score matplotlib pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librerie\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "\n",
    "# Seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione directory\n",
    "os.makedirs('./results_day4', exist_ok=True)\n",
    "print(\"‚úÖ Directory create\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Integrazione pipeline completa (40 minuti)\n",
    "\n",
    "Visualizziamo la pipeline completa con esempi concreti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline RLHF: schema visivo\n",
    "pipeline_stages = {\n",
    "    'Stage': ['Pre-training', 'SFT', 'Reward Model', 'RL Fine-tuning'],\n",
    "    'Input': [\n",
    "        'Testo raw (web, libri)',\n",
    "        'Esempi instruction-following',\n",
    "        'Coppie preferenze (chosen/rejected)',\n",
    "        'Prompt + reward model'\n",
    "    ],\n",
    "    'Obiettivo': [\n",
    "        'Next token prediction',\n",
    "        'Seguire istruzioni',\n",
    "        'Predire preferenze umane',\n",
    "        'Massimizzare reward'\n",
    "    ],\n",
    "    'Dataset Size': [\n",
    "        '100B-1T token',\n",
    "        '10K-100K esempi',\n",
    "        '10K-100K coppie',\n",
    "        'Generato on-the-fly'\n",
    "    ],\n",
    "    'Costo (USD)': [\n",
    "        '$1M-10M',\n",
    "        '$10K-100K',\n",
    "        '$50K-500K',\n",
    "        '$50K-500K'\n",
    "    ],\n",
    "    'Tempo': [\n",
    "        'Settimane-mesi',\n",
    "        'Ore-giorni',\n",
    "        'Giorni-settimane',\n",
    "        'Giorni-settimane'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_pipeline = pd.DataFrame(pipeline_stages)\n",
    "\n",
    "print(\"\\nüìä Pipeline RLHF Completa:\\n\")\n",
    "print(df_pipeline.to_string(index=False))\n",
    "\n",
    "df_pipeline.to_csv('./results_day4/pipeline_rlhf.csv', index=False)\n",
    "print(\"\\n‚úÖ Tabella salvata in ./results_day4/pipeline_rlhf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza pipeline come grafico\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "stages = df_pipeline['Stage'].tolist()\n",
    "y_pos = np.arange(len(stages))\n",
    "\n",
    "# Colori per stage\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "# Disegna box per ogni stage\n",
    "for i, (stage, color) in enumerate(zip(stages, colors)):\n",
    "    ax.add_patch(plt.Rectangle((0, i), 1, 0.8, facecolor=color, alpha=0.7, edgecolor='black', linewidth=2))\n",
    "    ax.text(0.5, i + 0.4, stage, ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
    "    \n",
    "    # Aggiungi frecce tra stage\n",
    "    if i < len(stages) - 1:\n",
    "        ax.arrow(0.5, i + 0.85, 0, 0.1, head_width=0.1, head_length=0.05, fc='black', ec='black', linewidth=2)\n",
    "\n",
    "ax.set_xlim(-0.2, 1.2)\n",
    "ax.set_ylim(-0.5, len(stages))\n",
    "ax.axis('off')\n",
    "ax.set_title('Pipeline RLHF: Pre-training ‚Üí SFT ‚Üí Reward Model ‚Üí RL', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day4/pipeline_diagram.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Diagramma pipeline salvato in ./results_day4/pipeline_diagram.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Esempio: Pipeline OpenAI\n",
    "\n",
    "**GPT-3 ‚Üí GPT-3.5 ‚Üí ChatGPT**:\n",
    "\n",
    "1. **GPT-3** (2020): pre-training su 300B token\n",
    "2. **GPT-3.5** (2022): SFT su ~13K esempi instruction-following\n",
    "3. **ChatGPT** (2022): RLHF con PPO\n",
    "   - Reward model trainato su ~33K preferenze\n",
    "   - PPO con migliaia di iterazioni\n",
    "   - Continuous improvement con user feedback\n",
    "\n",
    "**Risultato**: modello che segue istruzioni, rifiuta richieste inappropriate, ammette errori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Casi d'uso RLHF reali (30 minuti)\n",
    "\n",
    "Analizziamo applicazioni concrete di RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casi d'uso RLHF\n",
    "use_cases = {\n",
    "    'Applicazione': [\n",
    "        'Chatbot Customer Care',\n",
    "        'Assistenti Education',\n",
    "        'Code Generation',\n",
    "        'Content Moderation',\n",
    "        'Medical Q&A'\n",
    "    ],\n",
    "    'Obiettivo RLHF': [\n",
    "        'Cortesia, risoluzione problemi',\n",
    "        'Chiarezza, pedagogia',\n",
    "        'Correttezza, sicurezza',\n",
    "        'Identificare tossicit√†',\n",
    "        'Accuratezza, cautela'\n",
    "    ],\n",
    "    'Sfide': [\n",
    "        'Gestire clienti arrabbiati',\n",
    "        'Adattare a livello studente',\n",
    "        'Evitare vulnerabilit√†',\n",
    "        'Bias culturali',\n",
    "        'Responsabilit√† legale'\n",
    "    ],\n",
    "    'Esempi': [\n",
    "        'Zendesk AI, Intercom',\n",
    "        'Khan Academy, Duolingo',\n",
    "        'GitHub Copilot, Cursor',\n",
    "        'OpenAI Moderation API',\n",
    "        'Google Med-PaLM'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_use_cases = pd.DataFrame(use_cases)\n",
    "\n",
    "print(\"\\nüìä Casi d'Uso RLHF:\\n\")\n",
    "print(df_use_cases.to_string(index=False))\n",
    "\n",
    "df_use_cases.to_csv('./results_day4/rlhf_use_cases.csv', index=False)\n",
    "print(\"\\n‚úÖ Tabella salvata in ./results_day4/rlhf_use_cases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Focus: Chatbot Customer Care\n",
    "\n",
    "**Scenario**: e-commerce con 1M utenti/mese\n",
    "\n",
    "**Pipeline**:\n",
    "1. **Base model**: GPT-2 o LLaMA-7B\n",
    "2. **SFT**: fine-tune su 10K conversazioni customer care\n",
    "3. **Reward model**: train su 5K preferenze (cortesia, risoluzione)\n",
    "4. **PPO/DPO**: ottimizza per massimizzare soddisfazione cliente\n",
    "\n",
    "**Metriche di successo**:\n",
    "- **CSAT** (Customer Satisfaction): >80%\n",
    "- **Resolution rate**: >70%\n",
    "- **Escalation rate**: <20%\n",
    "- **Response time**: <5 secondi\n",
    "\n",
    "**Costi**:\n",
    "- Setup: $50K-100K\n",
    "- Maintenance: $10K-20K/anno\n",
    "- ROI: risparmio di $200K-500K/anno in agenti umani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ RLHF simulato su dataset preferenze (40 minuti)\n",
    "\n",
    "Simuliamo un training RLHF completo su dataset mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica modello base\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "\n",
    "print(f\"‚è≥ Caricamento {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model_base.eval()\n",
    "\n",
    "print(f\"‚úÖ Modello base caricato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simula modelli SFT e RLHF (per dimostrazione, usiamo stesso modello con noise)\n",
    "# In produzione, questi sarebbero modelli trainati separatamente\n",
    "\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model_sft.eval()\n",
    "\n",
    "model_rlhf = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model_rlhf.eval()\n",
    "\n",
    "print(\"‚úÖ Modelli SFT e RLHF simulati (per dimostrazione)\")\n",
    "print(\"   Nota: in produzione, questi sarebbero modelli trainati con Day 2 e Day 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto risposte Base vs SFT vs RLHF\n",
    "print(\"‚è≥ Generazione risposte...\\n\")\n",
    "\n",
    "# Prompt pi√π espliciti\n",
    "test_prompts = [\n",
    "    \"Question: How do I track my order? Answer:\",\n",
    "    \"Question: What is your return policy? Answer:\",\n",
    "    \"Question: Can I cancel my subscription? Answer:\",\n",
    "    \"Question: The product arrived damaged, what should I do? Answer:\",\n",
    "    \"Question: When will my refund be processed? Answer:\"\n",
    "]\n",
    "\n",
    "# Funzione di generazione MOLTO robusta\n",
    "def generate_clean(model, tokenizer, prompt, max_length=40):\n",
    "    \"\"\"Genera risposta pulita senza ripetizioni e whitespace\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            top_k=40,\n",
    "            repetition_penalty=2.0,  # Penalit√† molto forte\n",
    "            no_repeat_ngram_size=2,  # Evita anche 2-gram ripetuti\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decodifica\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Estrai risposta (dopo \"Answer:\")\n",
    "    if \"Answer:\" in full_text:\n",
    "        parts = full_text.split(\"Answer:\")\n",
    "        if len(parts) > 1:\n",
    "            response = parts[1].strip()\n",
    "        else:\n",
    "            response = full_text[len(prompt):].strip()\n",
    "    else:\n",
    "        response = full_text[len(prompt):].strip()\n",
    "    \n",
    "    # FILTRO AGGRESSIVO whitespace\n",
    "    import re\n",
    "    \n",
    "    # 1. Rimuovi TUTTI i newline\n",
    "    response = response.replace('\\n', ' ')\n",
    "    response = response.replace('\\r', ' ')\n",
    "    response = response.replace('\\t', ' ')\n",
    "    \n",
    "    # 2. Rimuovi spazi multipli\n",
    "    response = re.sub(r'\\s+', ' ', response)\n",
    "    \n",
    "    # 3. Rimuovi \"Question:\" se presente\n",
    "    response = response.replace('Question:', '')\n",
    "    \n",
    "    # 4. Tronca alla prima frase completa (punto, ?, !)\n",
    "    sentences = re.split(r'[.!?]', response)\n",
    "    if len(sentences) > 0 and sentences[0].strip():\n",
    "        response = sentences[0].strip()\n",
    "        # Aggiungi punto finale\n",
    "        if response and not response[-1] in '.!?':\n",
    "            response += '.'\n",
    "    \n",
    "    # 5. Limita lunghezza massima\n",
    "    if len(response) > 120:\n",
    "        response = response[:120].rsplit(' ', 1)[0] + '...'\n",
    "    \n",
    "    # 6. Fallback se vuoto\n",
    "    if not response or len(response) < 5:\n",
    "        response = \"[Nessuna risposta generata]\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Genera per ogni prompt\n",
    "for prompt in test_prompts:\n",
    "    # Estrai domanda\n",
    "    question = prompt.replace(\"Question: \", \"\").replace(\" Answer:\", \"\")\n",
    "    print(f\"üìù {question}\")\n",
    "    print()\n",
    "    \n",
    "    # Base\n",
    "    resp_base = generate_clean(model_base, tokenizer, prompt, max_length=30)\n",
    "    print(f\"   Base:  {resp_base}\")\n",
    "    \n",
    "    # SFT\n",
    "    resp_sft = generate_clean(model_sft, tokenizer, prompt, max_length=30)\n",
    "    print(f\"   SFT:   {resp_sft}\")\n",
    "    \n",
    "    # RLHF\n",
    "    resp_rlhf = generate_clean(model_rlhf, tokenizer, prompt, max_length=30)\n",
    "    print(f\"   RLHF:  {resp_rlhf}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Generazione completata\\n\")\n",
    "\n",
    "print(\"üí° Nota importante:\")\n",
    "print(\"   - GPT-2 (124M parametri) non √® trainato per Q&A customer service\")\n",
    "print(\"   - Le risposte possono essere generiche o non pertinenti\")\n",
    "print(\"   - In produzione: modelli 7B+ con fine-tuning specifico\")\n",
    "print(\"   - L'obiettivo √® mostrare il CONCETTO della pipeline, non qualit√† assoluta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per generare risposte\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"‚úÖ Funzione di generazione definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera risposte con tutti i modelli\n",
    "print(\"\\n‚è≥ Generazione risposte...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Base\n",
    "    base_response = generate_response(model_base, tokenizer, prompt)\n",
    "    print(f\"Base: {base_response}\\n\")\n",
    "    \n",
    "    # SFT\n",
    "    sft_response = generate_response(model_sft, tokenizer, prompt)\n",
    "    print(f\"SFT: {sft_response}\\n\")\n",
    "    \n",
    "    # RLHF\n",
    "    rlhf_response = generate_response(model_rlhf, tokenizer, prompt)\n",
    "    print(f\"RLHF: {rlhf_response}\\n\")\n",
    "    \n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    results.append({\n",
    "        'prompt': prompt,\n",
    "        'base': base_response,\n",
    "        'sft': sft_response,\n",
    "        'rlhf': rlhf_response\n",
    "    })\n",
    "\n",
    "print(\"‚úÖ Generazione completata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Confronto base vs SFT vs RLHF (40 minuti)\n",
    "\n",
    "Confrontiamo i tre modelli con valutazione qualitativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubrica di valutazione qualitativa\n",
    "rubric = {\n",
    "    'Criterio': ['Cortesia', 'Informativit√†', 'Risoluzione', 'Chiarezza', 'Sicurezza'],\n",
    "    'Descrizione': [\n",
    "        'Tono educato e professionale',\n",
    "        'Fornisce informazioni utili',\n",
    "        'Risolve il problema del cliente',\n",
    "        'Linguaggio chiaro e comprensibile',\n",
    "        'Non fornisce info dannose/false'\n",
    "    ],\n",
    "    'Punteggio': ['1-5', '1-5', '1-5', '1-5', '1-5']\n",
    "}\n",
    "\n",
    "df_rubric = pd.DataFrame(rubric)\n",
    "\n",
    "print(\"\\nüìä Rubrica di Valutazione Qualitativa:\\n\")\n",
    "print(df_rubric.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Istruzioni:\")\n",
    "print(\"   - Valuta ogni risposta con punteggio 1-5 per ogni criterio\")\n",
    "print(\"   - 1 = pessimo, 3 = accettabile, 5 = eccellente\")\n",
    "print(\"   - Calcola media per ottenere punteggio finale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di valutazione (simulata)\n",
    "# In classe, gli studenti farebbero questa valutazione manualmente\n",
    "\n",
    "evaluation_example = {\n",
    "    'Modello': ['Base', 'SFT', 'RLHF'],\n",
    "    'Cortesia': [2, 4, 5],\n",
    "    'Informativit√†': [2, 3, 4],\n",
    "    'Risoluzione': [1, 3, 4],\n",
    "    'Chiarezza': [3, 4, 5],\n",
    "    'Sicurezza': [3, 4, 5]\n",
    "}\n",
    "\n",
    "df_eval = pd.DataFrame(evaluation_example)\n",
    "df_eval['Media'] = df_eval[['Cortesia', 'Informativit√†', 'Risoluzione', 'Chiarezza', 'Sicurezza']].mean(axis=1)\n",
    "\n",
    "print(\"\\nüìä Esempio di Valutazione (simulata):\\n\")\n",
    "print(df_eval.to_string(index=False))\n",
    "\n",
    "# Visualizza\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(df_eval['Modello']))\n",
    "width = 0.15\n",
    "\n",
    "criteria = ['Cortesia', 'Informativit√†', 'Risoluzione', 'Chiarezza', 'Sicurezza']\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for i, (criterion, color) in enumerate(zip(criteria, colors)):\n",
    "    offset = width * (i - 2)\n",
    "    ax.bar(x + offset, df_eval[criterion], width, label=criterion, color=color, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Modello')\n",
    "ax.set_ylabel('Punteggio (1-5)')\n",
    "ax.set_title('Confronto Qualitativo: Base vs SFT vs RLHF')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_eval['Modello'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day4/qualitative_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Grafico salvato in ./results_day4/qualitative_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Osservazioni attese:\n",
    "\n",
    "1. **Base**: risposte generiche, a volte incoerenti, poco utili\n",
    "2. **SFT**: migliora struttura e informativit√†, ma pu√≤ mancare cortesia\n",
    "3. **RLHF**: ottimizza per preferenze umane, pi√π cortese e utile\n",
    "\n",
    "**Nota**: in questo esempio i modelli sono simulati. Con modelli realmente trainati, le differenze sarebbero pi√π marcate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Metriche automatiche (BLEU, ROUGE) (50 minuti)\n",
    "\n",
    "Implementiamo metriche automatiche per valutazione quantitativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Teoria: BLEU\n",
    "\n",
    "**BLEU** (Bilingual Evaluation Understudy) misura precision di n-grammi.\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "BLEU = BP √ó exp(Œ£ w_n log p_n)\n",
    "```\n",
    "Dove:\n",
    "- `p_n`: precision n-grammi (n=1,2,3,4)\n",
    "- `w_n`: peso (tipicamente 1/4 per ogni n)\n",
    "- `BP`: brevity penalty (penalizza risposte troppo corte)\n",
    "\n",
    "**Range**: 0-1 (o 0-100 se moltiplicato per 100)\n",
    "\n",
    "**Uso**: traduzione automatica, ma applicabile a generazione testo\n",
    "\n",
    "**Limiti**:\n",
    "- Non cattura semantica\n",
    "- Favorisce match esatti\n",
    "- Sensibile a sinonimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementazione BLEU\n",
    "def calculate_bleu(reference: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Calcola BLEU score.\n",
    "    \n",
    "    Args:\n",
    "        reference: testo di riferimento\n",
    "        candidate: testo generato\n",
    "    \n",
    "    Returns:\n",
    "        BLEU score (0-1)\n",
    "    \"\"\"\n",
    "    # Tokenizza\n",
    "    reference_tokens = reference.lower().split()\n",
    "    candidate_tokens = candidate.lower().split()\n",
    "    \n",
    "    # Smoothing per evitare 0\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Calcola BLEU\n",
    "    score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothing)\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Test\n",
    "ref = \"The cat is on the mat\"\n",
    "cand1 = \"The cat is on the mat\"  # Identico\n",
    "cand2 = \"The dog is on the mat\"  # Diverso\n",
    "cand3 = \"A cat sits on a mat\"    # Simile ma diverso\n",
    "\n",
    "print(\"\\nüß™ Test BLEU:\\n\")\n",
    "print(f\"Reference: {ref}\")\n",
    "print(f\"Candidate 1 (identico): {cand1}\")\n",
    "print(f\"   BLEU: {calculate_bleu(ref, cand1):.4f}\\n\")\n",
    "print(f\"Candidate 2 (1 parola diversa): {cand2}\")\n",
    "print(f\"   BLEU: {calculate_bleu(ref, cand2):.4f}\\n\")\n",
    "print(f\"Candidate 3 (simile): {cand3}\")\n",
    "print(f\"   BLEU: {calculate_bleu(ref, cand3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Teoria: ROUGE\n",
    "\n",
    "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) misura recall di n-grammi.\n",
    "\n",
    "**Varianti**:\n",
    "- **ROUGE-N**: overlap n-grammi\n",
    "- **ROUGE-L**: longest common subsequence\n",
    "- **ROUGE-W**: weighted longest common subsequence\n",
    "\n",
    "**Formula ROUGE-L**:\n",
    "```\n",
    "Recall = LCS(ref, cand) / len(ref)\n",
    "Precision = LCS(ref, cand) / len(cand)\n",
    "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "**Range**: 0-1\n",
    "\n",
    "**Uso**: summarization, ma applicabile a generazione testo\n",
    "\n",
    "**Differenza con BLEU**: ROUGE misura recall (quanto del reference √® catturato), BLEU misura precision (quanto del candidate √® corretto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementazione ROUGE\n",
    "def calculate_rouge(reference: str, candidate: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calcola ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        reference: testo di riferimento\n",
    "        candidate: testo generato\n",
    "    \n",
    "    Returns:\n",
    "        Dizionario con ROUGE-1, ROUGE-2, ROUGE-L\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "# Test\n",
    "print(\"\\nüß™ Test ROUGE:\\n\")\n",
    "print(f\"Reference: {ref}\")\n",
    "print(f\"Candidate 1 (identico): {cand1}\")\n",
    "rouge1 = calculate_rouge(ref, cand1)\n",
    "print(f\"   ROUGE-1: {rouge1['rouge1']:.4f}, ROUGE-2: {rouge1['rouge2']:.4f}, ROUGE-L: {rouge1['rougeL']:.4f}\\n\")\n",
    "\n",
    "print(f\"Candidate 2 (1 parola diversa): {cand2}\")\n",
    "rouge2 = calculate_rouge(ref, cand2)\n",
    "print(f\"   ROUGE-1: {rouge2['rouge1']:.4f}, ROUGE-2: {rouge2['rouge2']:.4f}, ROUGE-L: {rouge2['rougeL']:.4f}\\n\")\n",
    "\n",
    "print(f\"Candidate 3 (simile): {cand3}\")\n",
    "rouge3 = calculate_rouge(ref, cand3)\n",
    "print(f\"   ROUGE-1: {rouge3['rouge1']:.4f}, ROUGE-2: {rouge3['rouge2']:.4f}, ROUGE-L: {rouge3['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica metriche ai risultati generati\n",
    "# Usiamo come reference le risposte \"chosen\" del dataset di preferenze\n",
    "\n",
    "# Carica dataset preferenze\n",
    "import json\n",
    "\n",
    "with open('./data/preferences.jsonl', 'r') as f:\n",
    "    preferences = [json.loads(line) for line in f]\n",
    "\n",
    "# Calcola metriche per primi 5 esempi\n",
    "metrics_results = []\n",
    "\n",
    "for i, result in enumerate(results[:5]):\n",
    "    if i < len(preferences):\n",
    "        reference = preferences[i]['chosen']\n",
    "        \n",
    "        # BLEU\n",
    "        bleu_base = calculate_bleu(reference, result['base'])\n",
    "        bleu_sft = calculate_bleu(reference, result['sft'])\n",
    "        bleu_rlhf = calculate_bleu(reference, result['rlhf'])\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge_base = calculate_rouge(reference, result['base'])\n",
    "        rouge_sft = calculate_rouge(reference, result['sft'])\n",
    "        rouge_rlhf = calculate_rouge(reference, result['rlhf'])\n",
    "        \n",
    "        metrics_results.append({\n",
    "            'prompt': result['prompt'],\n",
    "            'bleu_base': bleu_base,\n",
    "            'bleu_sft': bleu_sft,\n",
    "            'bleu_rlhf': bleu_rlhf,\n",
    "            'rouge1_base': rouge_base['rouge1'],\n",
    "            'rouge1_sft': rouge_sft['rouge1'],\n",
    "            'rouge1_rlhf': rouge_rlhf['rouge1']\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_results)\n",
    "\n",
    "print(\"\\nüìä Metriche Automatiche:\\n\")\n",
    "print(df_metrics[['prompt', 'bleu_base', 'bleu_sft', 'bleu_rlhf']].to_string(index=False))\n",
    "\n",
    "# Media\n",
    "print(\"\\nüìä Media Metriche:\\n\")\n",
    "print(f\"BLEU Base: {df_metrics['bleu_base'].mean():.4f}\")\n",
    "print(f\"BLEU SFT: {df_metrics['bleu_sft'].mean():.4f}\")\n",
    "print(f\"BLEU RLHF: {df_metrics['bleu_rlhf'].mean():.4f}\")\n",
    "print(f\"\\nROUGE-1 Base: {df_metrics['rouge1_base'].mean():.4f}\")\n",
    "print(f\"ROUGE-1 SFT: {df_metrics['rouge1_sft'].mean():.4f}\")\n",
    "print(f\"ROUGE-1 RLHF: {df_metrics['rouge1_rlhf'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza metriche\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = ['Base', 'SFT', 'RLHF']\n",
    "bleu_scores = [\n",
    "    df_metrics['bleu_base'].mean(),\n",
    "    df_metrics['bleu_sft'].mean(),\n",
    "    df_metrics['bleu_rlhf'].mean()\n",
    "]\n",
    "rouge_scores = [\n",
    "    df_metrics['rouge1_base'].mean(),\n",
    "    df_metrics['rouge1_sft'].mean(),\n",
    "    df_metrics['rouge1_rlhf'].mean()\n",
    "]\n",
    "\n",
    "# Plot 1: BLEU\n",
    "axes[0].bar(models, bleu_scores, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('BLEU Score')\n",
    "axes[0].set_title('BLEU: Base vs SFT vs RLHF')\n",
    "axes[0].set_ylim(0, max(bleu_scores) * 1.2)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: ROUGE\n",
    "axes[1].bar(models, rouge_scores, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('ROUGE-1 F1')\n",
    "axes[1].set_title('ROUGE-1: Base vs SFT vs RLHF')\n",
    "axes[1].set_ylim(0, max(rouge_scores) * 1.2)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day4/automatic_metrics.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Grafici metriche salvati in ./results_day4/automatic_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Interpretazione metriche:\n",
    "\n",
    "**BLEU**:\n",
    "- **>0.5**: buona qualit√†\n",
    "- **0.3-0.5**: qualit√† media\n",
    "- **<0.3**: qualit√† bassa\n",
    "\n",
    "**ROUGE-1**:\n",
    "- **>0.5**: buon overlap\n",
    "- **0.3-0.5**: overlap medio\n",
    "- **<0.3**: overlap basso\n",
    "\n",
    "**Nota**: questi threshold sono indicativi e dipendono dal task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Discussione: valutazione automatica vs umana (20 minuti)\n",
    "\n",
    "### Pro metriche automatiche:\n",
    "- ‚úÖ **Velocit√†**: valutazione istantanea\n",
    "- ‚úÖ **Costo**: praticamente zero\n",
    "- ‚úÖ **Riproducibilit√†**: sempre stesso risultato\n",
    "- ‚úÖ **Scalabilit√†**: valuta milioni di esempi\n",
    "\n",
    "### Contro metriche automatiche:\n",
    "- ‚ùå **Non catturano semantica**: \"Il gatto mangia il topo\" vs \"Il topo √® mangiato dal gatto\"\n",
    "- ‚ùå **Non catturano coerenza**: testo grammaticalmente corretto ma insensato\n",
    "- ‚ùå **Non catturano utilit√†**: risposta corretta ma inutile per l'utente\n",
    "- ‚ùå **Bias verso match esatti**: penalizzano sinonimi e parafrasi\n",
    "\n",
    "### Pro valutazione umana:\n",
    "- ‚úÖ **Cattura semantica**: comprende significato\n",
    "- ‚úÖ **Cattura coerenza**: identifica nonsense\n",
    "- ‚úÖ **Cattura utilit√†**: valuta se risposta aiuta davvero\n",
    "- ‚úÖ **Flessibile**: pu√≤ valutare criteri complessi\n",
    "\n",
    "### Contro valutazione umana:\n",
    "- ‚ùå **Lenta**: ore/giorni per valutare dataset\n",
    "- ‚ùå **Costosa**: $1-10 per valutazione\n",
    "- ‚ùå **Soggettiva**: annotatori possono non essere d'accordo\n",
    "- ‚ùå **Non scalabile**: impossibile valutare milioni di esempi\n",
    "\n",
    "### Best practice: approccio combinato\n",
    "1. **Metriche automatiche** per screening rapido e monitoraggio continuo\n",
    "2. **Valutazione umana** su campione rappresentativo (es. 100-500 esempi)\n",
    "3. **Correlazione**: verifica che metriche automatiche correlino con giudizi umani\n",
    "4. **Iterazione**: usa feedback umano per migliorare reward model\n",
    "\n",
    "### Esempi industriali:\n",
    "- **Google**: usa BLEU per traduzione + valutazione umana su sample\n",
    "- **OpenAI**: usa metriche automatiche + human eval su benchmark\n",
    "- **Meta**: usa ROUGE per summarization + crowdsourcing per valutazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Esercizi TODO\n",
    "\n",
    "### Esercizio 1: Metriche aggiuntive\n",
    "Implementa BERTScore (usa embeddings invece di n-grammi) e confronta con BLEU/ROUGE.\n",
    "\n",
    "### Esercizio 2: Correlazione metriche-umani\n",
    "Fai valutare 10 risposte da 3 persone, calcola BLEU/ROUGE, e verifica correlazione.\n",
    "\n",
    "### Esercizio 3: Analisi per categoria\n",
    "Raggruppa prompt per categoria (tracking, return, cancel, etc.) e confronta metriche.\n",
    "\n",
    "### Esercizio 4: A/B test simulato\n",
    "Simula A/B test tra SFT e RLHF con metriche di business (CSAT, resolution rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 1 - Metriche aggiuntive\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 2 - Correlazione metriche-umani\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 3 - Analisi per categoria\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 4 - A/B test simulato\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Conclusione Day 4\n",
    "\n",
    "Oggi abbiamo:\n",
    "1. ‚úÖ Compreso la **pipeline RLHF completa**\n",
    "2. ‚úÖ Analizzato **casi d'uso reali** di RLHF\n",
    "3. ‚úÖ Simulato **RLHF su dataset preferenze**\n",
    "4. ‚úÖ Confrontato **base vs SFT vs RLHF** con valutazione qualitativa\n",
    "5. ‚úÖ Implementato **metriche automatiche** (BLEU, ROUGE)\n",
    "6. ‚úÖ Discusso **trade-off** tra valutazione automatica e umana\n",
    "\n",
    "**Prossimi passi (Day 5)**:\n",
    "- Perplessit√† (PPL)\n",
    "- lm-eval-harness per benchmark\n",
    "- SQuAD per QA evaluation\n",
    "- Valutazione umana in classe\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ File generati\n",
    "\n",
    "```\n",
    "./results_day4/\n",
    "‚îú‚îÄ‚îÄ pipeline_rlhf.csv\n",
    "‚îú‚îÄ‚îÄ pipeline_diagram.png\n",
    "‚îú‚îÄ‚îÄ rlhf_use_cases.csv\n",
    "‚îú‚îÄ‚îÄ qualitative_comparison.png\n",
    "‚îî‚îÄ‚îÄ automatic_metrics.png\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
