{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Day 1 ‚Äî Scaling Laws e Confronto Modelli\n",
    "\n",
    "**Obiettivo pratico (4 ore di codice)**: Comprendere le scaling laws, analizzare modelli di diverse dimensioni (DistilGPT-2, GPT-2 small, GPT-2 medium), confrontare parametri, memoria, velocit√† e qualit√† output.\n",
    "\n",
    "---\n",
    "\n",
    "## üó∫Ô∏è Roadmap della lezione\n",
    "\n",
    "| **Sezione** | **Contenuto** | **Tempo stimato** |\n",
    "|-------------|---------------|-------------------|\n",
    "| 1 | Setup e utility \n",
    "| 2 | Scaling laws: teoria e visualizzazioni \n",
    "| 3 | Caricamento modelli (DistilGPT2, GPT2 small, GPT2 medium) \n",
    "| 4 | Analisi memoria e parametri \n",
    "| 5 | Confronto output su prompt identici \n",
    "| 6 | Analisi tempo inferenza e trade-off \n",
    "| 7 | Discussione: pre-training vs riuso \n",
    "\n",
    "---\n",
    "\n",
    "## üìö Parole chiave (per approfondire)\n",
    "\n",
    "- **Scaling laws (Kaplan et al.)**: relazioni empiriche tra parametri, dati, compute\n",
    "- **GPT-3**: architettura, dataset, limiti\n",
    "- **LLaMA**: filosofia open ed efficienza\n",
    "- **Falcon**: ottimizzazioni specifiche\n",
    "- **Training distribuito**: data parallelism vs model parallelism\n",
    "- **Mixed precision training**: FP16, BF16\n",
    "- **Gradient checkpointing**: trade-off memoria vs compute\n",
    "- **DeepSpeed e Megatron**: framework per modelli grandi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup e utility (15 minuti)\n",
    "\n",
    "Prepariamo l'ambiente con le librerie necessarie e utility per monitorare risorse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione librerie\n",
    "%pip install torch transformers matplotlib pandas numpy psutil ipywidgets pandas hf_xet -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo l'installazione, se appare il messaggio che chiede di riavviare il kernel, dalla toolbar in alto selezionare **\"Kernel\"** e in seguito **\"Restart Kernel and Clear Outputs of All Cells...\"** \n",
    "\n",
    "**Attendere circa 30 secondi e in seguito NON eseguire nuovamente il blocco \"Installazione librerie\" ma procedere con il blocco successivo, \"Import librerie\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librerie\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import set_seed, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_torch_available\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed per riproducibilit√†\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"‚úÖ Trasformers version: {transformers.__version__}\")\n",
    "print(f\"‚úÖ Is Torch Available: {is_torch_available()}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"‚úÖ Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility per monitorare RAM\n",
    "def get_memory_usage():\n",
    "    \"\"\"\n",
    "    Restituisce l'uso corrente di memoria RAM in MB.\n",
    "    \n",
    "    Returns:\n",
    "        float: Memoria RAM usata in MB\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # Converti in MB\n",
    "\n",
    "# Test\n",
    "current_mem = get_memory_usage()\n",
    "print(f\"üß™ Memoria RAM corrente: {current_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility per timing\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    Context manager per misurare il tempo di esecuzione.\n",
    "    \n",
    "    Uso:\n",
    "        with Timer(\"operazione\"):\n",
    "            # codice da misurare\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.elapsed = self.end - self.start\n",
    "        print(f\"‚è±Ô∏è {self.name}: {self.elapsed:.3f}s\")\n",
    "\n",
    "# Test\n",
    "with Timer(\"Test timer\"):\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "print(\"\\n‚úÖ Utility caricate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione directory\n",
    "os.makedirs('./results_day1', exist_ok=True)\n",
    "print(\"‚úÖ Directory ./results_day1 creata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Scaling Laws: teoria e visualizzazioni (45 minuti)\n",
    "\n",
    "Le **scaling laws** (Kaplan et al., 2020) descrivono relazioni empiriche tra:\n",
    "- **N**: numero di parametri del modello\n",
    "- **D**: dimensione del dataset (numero di token)\n",
    "- **C**: compute (FLOPs, numero di operazioni in virgola mobile, per training)\n",
    "- **L**: loss (performance del modello)\n",
    "\n",
    "### Relazioni chiave:\n",
    "\n",
    "**1. Loss vs Parametri**:\n",
    "```\n",
    "L(N) ‚àù N^(-Œ±)\n",
    "```\n",
    "Dove Œ± ‚âà 0.076 (da paper Kaplan et al.)\n",
    "\n",
    "Questa relazione descrive come la loss di un modello linguistico decresca al crescere del numero di parametri \n",
    "ùëÅ\n",
    "N. In altre parole, modelli pi√π grandi tendono a ottenere prestazioni migliori (loss pi√π bassa), ma con un miglioramento che segue una legge di potenza decrescente: ogni raddoppio dei parametri porta a un guadagno sempre pi√π marginale. Il valore di Œ± quantifica la rapidit√† con cui si riducono gli errori all‚Äôaumentare della capacit√† del modello.\n",
    "\n",
    "**2. Loss vs Dataset Size**:\n",
    "```\n",
    "L(D) ‚àù D^(-Œ≤)\n",
    "```\n",
    "Dove Œ≤ ‚âà 0.095\n",
    "\n",
    "Questa relazione mostra l‚Äôimpatto della dimensione del dataset di addestramento \n",
    "ùê∑\n",
    "D sulla perdita del modello. Aumentare i dati migliora l‚Äôapprendimento e riduce la loss, ma anche qui con rendimenti decrescenti: servono quantit√† sempre maggiori di dati per ottenere miglioramenti marginali. Il valore di Œ≤ rappresenta quanto ‚Äúefficientemente‚Äù il modello sfrutta nuovi esempi; un Œ≤ pi√π alto indica una maggiore sensibilit√† alla quantit√† di dati disponibili.\n",
    "\n",
    "**3. Loss vs Compute**:\n",
    "```\n",
    "L(C) ‚àù C^(-Œ≥)\n",
    "```\n",
    "Dove Œ≥ ‚âà 0.050\n",
    "\n",
    "Questa curva lega la loss alla quantit√† complessiva di calcolo \n",
    "ùê∂\n",
    "C utilizzata durante l‚Äôaddestramento (che dipende sia dal numero di parametri che dalla dimensione dei dati). Il parametro Œ≥ evidenzia che, pur investendo pi√π potenza computazionale, il miglioramento nella loss cresce lentamente: la riduzione degli errori segue una scala sub-lineare, riflettendo il costo crescente di addestrare modelli sempre pi√π grandi e complessi.\n",
    "\n",
    "### Implicazioni:\n",
    "- Aumentare parametri **e** dati insieme √® pi√π efficace\n",
    "- Raddoppiare i parametri riduce la loss di ~5%\n",
    "- Esiste un trade-off ottimale tra N e D per un dato budget C\n",
    "\n",
    "### Esempi concreti:\n",
    "- **GPT-3** (175B parametri): 300B token, ~3.14e23 FLOPs\n",
    "- **LLaMA-7B**: 1T token (pi√π dati, meno parametri)\n",
    "- **Falcon-40B**: 1T token, ottimizzazioni architetturali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione scaling laws\n",
    "\n",
    "# Parametri da paper Kaplan et al.\n",
    "alpha = 0.076  # Esponente per parametri\n",
    "beta = 0.095   # Esponente per dataset\n",
    "gamma = 0.050  # Esponente per compute\n",
    "\n",
    "# Range di valori (scala logaritmica)\n",
    "N_range = np.logspace(6, 11, 50)  # 1M a 100B parametri\n",
    "D_range = np.logspace(8, 12, 50)  # 100M a 1T token\n",
    "C_range = np.logspace(18, 24, 50) # FLOPs\n",
    "\n",
    "# Calcola loss (normalizzata)\n",
    "L_N = N_range ** (-alpha)\n",
    "L_D = D_range ** (-beta)\n",
    "L_C = C_range ** (-gamma)\n",
    "\n",
    "# Normalizza per visualizzazione\n",
    "L_N = L_N / L_N[0] * 10\n",
    "L_D = L_D / L_D[0] * 10\n",
    "L_C = L_C / L_C[0] * 10\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Loss vs Parametri\n",
    "axes[0].loglog(N_range, L_N, linewidth=2, color='#3498db')\n",
    "axes[0].set_xlabel('Numero Parametri (N)')\n",
    "axes[0].set_ylabel('Loss (normalizzata)')\n",
    "axes[0].set_title('Scaling Law: Loss vs Parametri')\n",
    "axes[0].grid(True, alpha=0.3, which='both')\n",
    "axes[0].axvline(117e6, color='red', linestyle='--', alpha=0.5, label='GPT-2 small (117M)')\n",
    "axes[0].axvline(345e6, color='orange', linestyle='--', alpha=0.5, label='GPT-2 medium (345M)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Loss vs Dataset\n",
    "axes[1].loglog(D_range, L_D, linewidth=2, color='#2ecc71')\n",
    "axes[1].set_xlabel('Dataset Size (token)')\n",
    "axes[1].set_ylabel('Loss (normalizzata)')\n",
    "axes[1].set_title('Scaling Law: Loss vs Dataset')\n",
    "axes[1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 3: Loss vs Compute\n",
    "axes[2].loglog(C_range, L_C, linewidth=2, color='#e74c3c')\n",
    "axes[2].set_xlabel('Compute (FLOPs)')\n",
    "axes[2].set_ylabel('Loss (normalizzata)')\n",
    "axes[2].set_title('Scaling Law: Loss vs Compute')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day1/scaling_laws.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úÖ Grafici scaling laws salvati in ./results_day1/scaling_laws.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Interpretazione dei grafici delle Scaling Laws\n",
    "\n",
    "I tre grafici mostrano l‚Äôandamento teorico della **loss** in funzione di tre variabili chiave che determinano la qualit√† e la scalabilit√† dei modelli linguistici:\n",
    "\n",
    "1. **Loss vs Numero di Parametri (N)**  \n",
    "   La curva blu mostra che, all‚Äôaumentare dei parametri del modello, la loss diminuisce secondo una legge di potenza.  \n",
    "   Tuttavia, la pendenza √® modesta (Œ± ‚âà 0.076): raddoppiare i parametri riduce la loss solo di una piccola frazione.  \n",
    "   Le linee tratteggiate indicano due configurazioni note di GPT-2 (*small* e *medium*), fornendo un riferimento concreto sulle scale tipiche dei modelli.  \n",
    "\n",
    "2. **Loss vs Dimensione del Dataset (D)**  \n",
    "   La curva verde evidenzia come l‚Äôaumento del numero di token di addestramento riduca la loss in modo simile.  \n",
    "   Anche qui si osservano rendimenti decrescenti (Œ≤ ‚âà 0.095): servono dataset sempre pi√π grandi per ottenere miglioramenti marginali.  \n",
    "   Questo conferma l‚Äôimportanza di bilanciare quantit√† e qualit√† dei dati durante il training.  \n",
    "\n",
    "3. **Loss vs Compute Totale (C)**  \n",
    "   La curva rossa mostra la dipendenza della loss dal compute impiegato (misurato in FLOPs).  \n",
    "   L‚Äôesponente Œ≥ ‚âà 0.050 √® il pi√π piccolo dei tre, indicando che incrementare la potenza di calcolo porta benefici ancora pi√π lenti.  \n",
    "   Ci√≤ riflette il costo crescente di addestrare modelli pi√π grandi e complessi rispetto ai guadagni in accuratezza.  \n",
    "\n",
    "---\n",
    "\n",
    "Nel complesso, i grafici illustrano la natura **sub-lineare e a rendimenti decrescenti** delle scaling laws: ogni ordine di grandezza in pi√π di risorse (parametri, dati o calcolo) produce miglioramenti sempre minori nella loss.  \n",
    "Queste relazioni sono fondamentali per comprendere i limiti pratici della scalabilit√† dei modelli di linguaggio e per pianificare in modo ottimale l‚Äôuso di risorse computazionali e dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiegazione approfondita del codice \n",
    "\n",
    "Il codice mostra come visualizzare le scaling laws dei modelli linguistici, cio√® le relazioni empiriche tra la loss e tre variabili fondamentali: numero di parametri, dimensione del dataset e potenza di calcolo.\n",
    "\n",
    "**1) Nelle prime righe vengono importati gli esponenti Œ±, Œ≤ e Œ≥ ricavati dal paper di Kaplan et al. (2020), che descrivono quanto velocemente decresce la loss rispetto a ciascun fattore:**\n",
    "\n",
    "alpha = 0.076  # Esponente per parametri  \n",
    "beta = 0.095   # Esponente per dataset  \n",
    "gamma = 0.050  # Esponente per compute\n",
    "\n",
    "**2) Successivamente, si definiscono i range di valori su scala logaritmica per ciascuna variabile:**\n",
    "\n",
    "N_range = np.logspace(6, 11, 50)  # Da 1 milione a 100 miliardi di parametri  \n",
    "D_range = np.logspace(8, 12, 50)  # Da 100 milioni a 1 trilione di token  \n",
    "C_range = np.logspace(18, 24, 50) # Da 10¬π‚Å∏ a 10¬≤‚Å¥ FLOPs\n",
    "\n",
    "\n",
    "L‚Äôuso di np.logspace consente di coprire ordini di grandezza molto ampi, come tipico nei modelli di deep learning.\n",
    "\n",
    "**3) Le tre righe successive calcolano la loss teorica in funzione di ciascun fattore, seguendo le leggi di potenza viste in precedenza:**\n",
    "\n",
    "L_N = N_range ** (-alpha)  \n",
    "L_D = D_range ** (-beta)  \n",
    "L_C = C_range ** (-gamma)\n",
    "\n",
    "**4) Poich√© l‚Äôobiettivo √® una rappresentazione qualitativa, le loss vengono poi normalizzate per renderle confrontabili e graficamente leggibili:**\n",
    "\n",
    "L_N = L_N / L_N[0] * 10  \n",
    "L_D = L_D / L_D[0] * 10  \n",
    "L_C = L_C / L_C[0] * 10\n",
    "\n",
    "In questo modo tutti i grafici partono da un valore di loss pari a 10 e diminuiscono secondo la legge di scaling.\n",
    "\n",
    "**5) Si crea quindi una figura con tre sottotrame (subplots) affiancate:**\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "\n",
    "Ognuna di esse rappresenta una delle tre relazioni fondamentali.\n",
    "\n",
    "Nel primo grafico (Loss vs Parametri), la curva blu mostra come la loss diminuisca all‚Äôaumentare del numero di parametri. Le linee verticali tratteggiate in rosso e arancione indicano la posizione approssimativa di GPT-2 small (117M) e GPT-2 medium (345M), fornendo un riferimento concreto.**\n",
    "\n",
    "Nel secondo grafico (Loss vs Dataset), la curva verde evidenzia l‚Äôimpatto della dimensione del dataset sui risultati: pi√π dati migliorano la performance, ma con rendimenti decrescenti.\n",
    "\n",
    "Nel terzo grafico (Loss vs Compute), la curva rossa mostra l‚Äôeffetto dell‚Äôaumento del calcolo (FLOPs) sull‚Äôerrore: anche qui i miglioramenti si attenuano progressivamente.\n",
    "\n",
    "**6) Infine, il codice ottimizza la disposizione dei grafici, salva l‚Äôimmagine nella cartella ./results_day1/ e la visualizza a schermo:**\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day1/scaling_laws.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "**7) La stampa finale conferma l‚Äôavvenuto salvataggio:**\n",
    "\n",
    "print(\"‚úÖ Grafici scaling laws salvati in ./results_day1/scaling_laws.png\")\n",
    "\n",
    "\n",
    "In sintesi, questo script permette di visualizzare in modo intuitivo come la loss diminuisca secondo leggi di potenza man mano che aumentano i parametri, i dati e la potenza di calcolo, evidenziando la natura scalabile ma con rendimenti decrescenti dei moderni modelli di linguaggio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto modelli famosi\n",
    "models_comparison = {\n",
    "    'Modello': ['GPT-2 small', 'GPT-2 medium', 'GPT-2 large', 'GPT-3', 'LLaMA-7B', 'LLaMA-13B', 'Falcon-7B', 'Falcon-40B'],\n",
    "    'Parametri': ['117M', '345M', '762M', '175B', '7B', '13B', '7B', '40B'],\n",
    "    'Dataset (token)': ['~40B', '~40B', '~40B', '300B', '1T', '1T', '1T', '1T'],\n",
    "    'Anno': [2019, 2019, 2019, 2020, 2023, 2023, 2023, 2023],\n",
    "    'Open Source': ['S√¨', 'S√¨', 'S√¨', 'No', 'S√¨', 'S√¨', 'S√¨', 'S√¨'],\n",
    "    'Note': [\n",
    "        'Baseline, CPU-friendly',\n",
    "        'Buon trade-off',\n",
    "        'Richiede GPU',\n",
    "        'Closed, API only',\n",
    "        'Efficiente, open',\n",
    "        'Pi√π potente',\n",
    "        'Ottimizzato',\n",
    "        'SOTA open'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_models = pd.DataFrame(models_comparison)\n",
    "\n",
    "print(\"\\nüìä Confronto Modelli Famosi:\\n\")\n",
    "print(df_models.to_string(index=False))\n",
    "\n",
    "# Salva\n",
    "df_models.to_csv('./results_day1/models_comparison.csv', index=False)\n",
    "print(\"\\n‚úÖ Tabella salvata in ./results_day1/models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Osservazioni chiave:\n",
    "\n",
    "1. **GPT-3 vs LLaMA**: LLaMA usa molti pi√π dati (1T vs 300B token) con meno parametri ‚Üí pi√π efficiente\n",
    "2. **Trend recente**: focus su dataset grandi piuttosto che solo parametri\n",
    "3. **Open source**: LLaMA e Falcon democratizzano l'accesso a modelli potenti\n",
    "4. **Trade-off**: GPT-2 small √® ancora utile per prototipazione e CPU\n",
    "\n",
    "### Limiti del pre-training:\n",
    "- **Allucinazioni**: il modello genera testo plausibile ma falso\n",
    "- **Dataset rumorosi**: qualit√† variabile dei dati web\n",
    "- **Aggiornamento lento**: modelli non conoscono eventi recenti\n",
    "- **Costi**: training GPT-3 costa ~$4.6M in compute\n",
    "- **Energia**: impatto ambientale significativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Caricamento modelli (30 minuti)\n",
    "\n",
    "Carichiamo tre modelli della famiglia GPT-2:\n",
    "- **DistilGPT-2**: 82M parametri (distillato da GPT-2)\n",
    "- **GPT-2 small**: 117M parametri (originale)\n",
    "- **GPT-2 medium**: 345M parametri (3x pi√π grande)\n",
    "\n",
    "Tutti sono **CPU-friendly** e disponibili su Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione modelli da caricare\n",
    "MODEL_NAMES = {\n",
    "    'distilgpt2': 'DistilGPT-2 (82M)',\n",
    "    'gpt2': 'GPT-2 small (117M)',\n",
    "    'gpt2-medium': 'GPT-2 medium (345M)'\n",
    "}\n",
    "\n",
    "print(\"üì¶ Modelli da caricare:\")\n",
    "for key, name in MODEL_NAMES.items():\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento modelli e tokenizer\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "print(\"\\n‚è≥ Caricamento modelli in corso...\\n\")\n",
    "\n",
    "for model_key, model_name in MODEL_NAMES.items():\n",
    "    print(f\"Caricamento {model_name}...\")\n",
    "    \n",
    "    mem_before = get_memory_usage()\n",
    "    \n",
    "    with Timer(f\"  Tempo caricamento\"):\n",
    "        # Carica tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_key)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Carica modello\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_key)\n",
    "        model.eval()  # Modalit√† evaluation\n",
    "    \n",
    "    mem_after = get_memory_usage()\n",
    "    mem_used = mem_after - mem_before\n",
    "    \n",
    "    print(f\"  Memoria usata: {mem_used:.2f} MB\\n\")\n",
    "    \n",
    "    models[model_key] = model\n",
    "    tokenizers[model_key] = tokenizer\n",
    "\n",
    "print(\"‚úÖ Tutti i modelli caricati!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Analisi memoria e parametri (40 minuti)\n",
    "\n",
    "Analizziamo in dettaglio:\n",
    "- Numero di parametri totali\n",
    "- Numero di parametri trainable\n",
    "- Dimensione del modello su disco\n",
    "- Memoria RAM richiesta\n",
    "- Struttura dei layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per analizzare parametri\n",
    "def analyze_model_parameters(model, model_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analizza i parametri di un modello.\n",
    "    \n",
    "    Args:\n",
    "        model: modello PyTorch\n",
    "        model_name: nome del modello\n",
    "    \n",
    "    Returns:\n",
    "        Dizionario con statistiche\n",
    "    \"\"\"\n",
    "    # Conta parametri totali\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Conta parametri trainable\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    # Stima dimensione in MB (assumendo FP32)\n",
    "    # Ogni parametro FP32 = 4 bytes\n",
    "    size_mb = (total_params * 4) / (1024 ** 2)\n",
    "    \n",
    "    # Conta layer\n",
    "    num_layers = len(list(model.named_modules()))\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'size_mb': size_mb,\n",
    "        'num_layers': num_layers\n",
    "    }\n",
    "\n",
    "# Analizza tutti i modelli\n",
    "model_stats = []\n",
    "\n",
    "for model_key, model_name in MODEL_NAMES.items():\n",
    "    stats = analyze_model_parameters(models[model_key], model_name)\n",
    "    model_stats.append(stats)\n",
    "\n",
    "df_stats = pd.DataFrame(model_stats)\n",
    "\n",
    "print(\"\\nüìä Statistiche Modelli:\\n\")\n",
    "print(df_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione parametri\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Numero parametri\n",
    "models_names = [m['model'] for m in model_stats]\n",
    "params_millions = [m['total_params'] / 1e6 for m in model_stats]\n",
    "\n",
    "axes[0].bar(models_names, params_millions, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Parametri (Milioni)')\n",
    "axes[0].set_title('Numero di Parametri per Modello')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Aggiungi valori sopra le barre\n",
    "for i, (name, value) in enumerate(zip(models_names, params_millions)):\n",
    "    axes[0].text(i, value + 5, f\"{value:.1f}M\", ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Dimensione modello\n",
    "sizes_mb = [m['size_mb'] for m in model_stats]\n",
    "\n",
    "axes[1].bar(models_names, sizes_mb, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Dimensione (MB)')\n",
    "axes[1].set_title('Dimensione Modello (FP32)')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Aggiungi valori\n",
    "for i, (name, value) in enumerate(zip(models_names, sizes_mb)):\n",
    "    axes[1].text(i, value + 20, f\"{value:.0f}MB\", ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day1/model_parameters.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Grafici parametri salvati in ./results_day1/model_parameters.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi struttura layer (esempio su GPT-2 small)\n",
    "print(\"\\nüîç Struttura layer GPT-2 small:\\n\")\n",
    "\n",
    "gpt2_model = models['gpt2']\n",
    "\n",
    "# Conta parametri per tipo di layer\n",
    "layer_params = {}\n",
    "\n",
    "for name, param in gpt2_model.named_parameters():\n",
    "    # Estrai tipo di layer (prima parte del nome)\n",
    "    layer_type = name.split('.')[0]\n",
    "    \n",
    "    if layer_type not in layer_params:\n",
    "        layer_params[layer_type] = 0\n",
    "    \n",
    "    layer_params[layer_type] += param.numel()\n",
    "\n",
    "# Ordina per numero parametri\n",
    "layer_params_sorted = sorted(layer_params.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Parametri per tipo di layer:\")\n",
    "for layer_type, num_params in layer_params_sorted:\n",
    "    percentage = (num_params / df_stats[df_stats['model'] == 'GPT-2 small (117M)']['total_params'].values[0]) * 100\n",
    "    print(f\"   {layer_type}: {num_params/1e6:.2f}M ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Osservazioni:\n",
    "\n",
    "1. **DistilGPT-2 vs GPT-2**: 30% meno parametri, ma performance simili (distillazione efficace)\n",
    "2. **GPT-2 medium**: 3x parametri ‚Üí 3x dimensione ‚Üí 3x memoria\n",
    "3. **Scaling lineare**: parametri e dimensione crescono linearmente\n",
    "4. **Layer transformer**: la maggior parte dei parametri √® nei layer attention e feedforward\n",
    "\n",
    "### Implicazioni pratiche:\n",
    "- **CPU**: DistilGPT-2 e GPT-2 small sono gestibili\n",
    "- **GPU**: GPT-2 medium richiede almeno 2GB VRAM\n",
    "- **Produzione**: considerare quantizzazione per ridurre dimensione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Confronto output su prompt identici (50 minuti)\n",
    "\n",
    "Testiamo i tre modelli su prompt identici per confrontare:\n",
    "- Qualit√† del testo generato\n",
    "- Coerenza e fluenza\n",
    "- Capacit√† di seguire istruzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per generare testo\n",
    "def generate_text(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Genera testo dato un prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: modello generativo\n",
    "        tokenizer: tokenizer\n",
    "        prompt: prompt di input\n",
    "        max_new_tokens: numero massimo di token da generare\n",
    "        temperature: temperatura per sampling (0 = greedy, >1 = pi√π random)\n",
    "        top_p: nucleus sampling threshold\n",
    "    \n",
    "    Returns:\n",
    "        Testo generato completo (prompt + generazione)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"‚úÖ Funzione di generazione definita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt di test\n",
    "test_prompts = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"The future of technology will\",\n",
    "    \"Machine learning models can\",\n",
    "    \"In the year 2030, we will\",\n",
    "    \"The most important challenge in AI is\"\n",
    "]\n",
    "\n",
    "print(\"üìù Prompt di test:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"   {i}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera output per tutti i modelli\n",
    "print(\"\\n‚è≥ Generazione output in corso...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for model_key, model_name in MODEL_NAMES.items():\n",
    "        print(f\"ü§ñ {model_name}:\")\n",
    "        \n",
    "        with Timer(\"   Tempo generazione\"):\n",
    "            output = generate_text(\n",
    "                models[model_key],\n",
    "                tokenizers[model_key],\n",
    "                prompt,\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        # Estrai solo la parte generata (rimuovi prompt)\n",
    "        generated_part = output[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"   Output: {generated_part}\\n\")\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'model': model_name,\n",
    "            'output': generated_part\n",
    "        })\n",
    "\n",
    "print(\"\\n‚úÖ Generazione completata!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva risultati\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('./results_day1/generation_outputs.csv', index=False)\n",
    "print(\"‚úÖ Risultati salvati in ./results_day1/generation_outputs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Analisi qualitativa:\n",
    "\n",
    "**Cosa osservare negli output**:\n",
    "1. **Coerenza**: il testo ha senso logico?\n",
    "2. **Fluenza**: la grammatica √® corretta?\n",
    "3. **Creativit√†**: il modello genera idee originali o ripete pattern?\n",
    "4. **Lunghezza**: il modello rispetta il limite di token?\n",
    "\n",
    "**Differenze attese**:\n",
    "- **DistilGPT-2**: pi√π semplice, a volte meno coerente\n",
    "- **GPT-2 small**: buon bilanciamento\n",
    "- **GPT-2 medium**: pi√π articolato, vocabolario pi√π ricco\n",
    "\n",
    "**Nota**: tutti e tre i modelli possono generare **allucinazioni** (fatti inventati ma plausibili)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Analisi tempo inferenza e trade-off (40 minuti)\n",
    "\n",
    "Misuriamo il tempo di inferenza per valutare il trade-off **qualit√† vs velocit√†**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inferenza\n",
    "def benchmark_inference(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    num_runs: int = 5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Misura tempo medio di inferenza.\n",
    "    \n",
    "    Args:\n",
    "        model: modello\n",
    "        tokenizer: tokenizer\n",
    "        prompt: prompt di test\n",
    "        num_runs: numero di esecuzioni per media\n",
    "    \n",
    "    Returns:\n",
    "        Dizionario con statistiche\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        _ = generate_text(model, tokenizer, prompt, max_new_tokens=30, temperature=0.7)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times)\n",
    "    }\n",
    "\n",
    "print(\"‚è≥ Benchmark inferenza in corso...\\n\")\n",
    "\n",
    "benchmark_prompt = \"Artificial intelligence is\"\n",
    "benchmark_results = []\n",
    "\n",
    "for model_key, model_name in MODEL_NAMES.items():\n",
    "    print(f\"Benchmark {model_name}...\")\n",
    "    \n",
    "    stats = benchmark_inference(\n",
    "        models[model_key],\n",
    "        tokenizers[model_key],\n",
    "        benchmark_prompt,\n",
    "        num_runs=5\n",
    "    )\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        'model': model_name,\n",
    "        'mean_time': stats['mean'],\n",
    "        'std_time': stats['std']\n",
    "    })\n",
    "    \n",
    "    print(f\"   Tempo medio: {stats['mean']:.3f}s ¬± {stats['std']:.3f}s\\n\")\n",
    "\n",
    "df_benchmark = pd.DataFrame(benchmark_results)\n",
    "\n",
    "print(\"‚úÖ Benchmark completato!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza trade-off qualit√† vs velocit√†\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Tempo inferenza\n",
    "models_names = df_benchmark['model'].tolist()\n",
    "mean_times = df_benchmark['mean_time'].tolist()\n",
    "std_times = df_benchmark['std_time'].tolist()\n",
    "\n",
    "axes[0].bar(models_names, mean_times, yerr=std_times, color=['#3498db', '#2ecc71', '#e74c3c'], \n",
    "            alpha=0.7, edgecolor='black', capsize=5)\n",
    "axes[0].set_ylabel('Tempo (secondi)')\n",
    "axes[0].set_title('Tempo Medio di Inferenza')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 2: Trade-off parametri vs velocit√†\n",
    "params = [m['total_params'] / 1e6 for m in model_stats]\n",
    "times = mean_times\n",
    "\n",
    "axes[1].scatter(params, times, s=200, alpha=0.6, c=['#3498db', '#2ecc71', '#e74c3c'], edgecolors='black', linewidths=2)\n",
    "axes[1].set_xlabel('Parametri (Milioni)')\n",
    "axes[1].set_ylabel('Tempo Inferenza (secondi)')\n",
    "axes[1].set_title('Trade-off: Parametri vs Velocit√†')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Annota punti\n",
    "for i, name in enumerate(models_names):\n",
    "    axes[1].annotate(name.split('(')[0].strip(), (params[i], times[i]), \n",
    "                     xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./results_day1/inference_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Grafici benchmark salvati in ./results_day1/inference_benchmark.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola speedup relativo\n",
    "baseline_time = df_benchmark[df_benchmark['model'] == 'GPT-2 medium (345M)']['mean_time'].values[0]\n",
    "\n",
    "print(\"\\nüìä Speedup relativo (vs GPT-2 medium):\\n\")\n",
    "for idx, row in df_benchmark.iterrows():\n",
    "    speedup = baseline_time / row['mean_time']\n",
    "    print(f\"   {row['model']}: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Osservazioni:\n",
    "\n",
    "1. **Scaling lineare**: tempo di inferenza scala linearmente con i parametri\n",
    "2. **DistilGPT-2**: ~3x pi√π veloce di GPT-2 medium\n",
    "3. **Trade-off**: GPT-2 medium √® pi√π lento ma genera testo pi√π articolato\n",
    "4. **CPU**: tutti e tre sono gestibili su CPU per prototipazione\n",
    "\n",
    "### Quando scegliere quale:\n",
    "- **DistilGPT-2**: prototipazione rapida, deployment su edge devices\n",
    "- **GPT-2 small**: buon bilanciamento per la maggior parte dei casi\n",
    "- **GPT-2 medium**: quando serve qualit√† massima e velocit√† non √® critica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Discussione: pre-training vs riuso (20 minuti)\n",
    "\n",
    "### Quando fare pre-training da zero?\n",
    "\n",
    "**PRO**:\n",
    "- Controllo completo su dati e architettura\n",
    "- Possibilit√† di ottimizzare per dominio specifico\n",
    "- Nessuna dipendenza da modelli esterni\n",
    "\n",
    "**CONTRO**:\n",
    "- Costi enormi (compute, energia, tempo)\n",
    "- Richiede dataset giganteschi (centinaia di miliardi di token)\n",
    "- Expertise tecnico avanzato\n",
    "- Rischio di risultati peggiori di modelli esistenti\n",
    "\n",
    "### Quando riusare modelli esistenti?\n",
    "\n",
    "**PRO**:\n",
    "- Costi ridotti (solo fine-tuning)\n",
    "- Risultati rapidi\n",
    "- Beneficia di anni di ricerca\n",
    "- Modelli open source disponibili (LLaMA, Falcon, etc.)\n",
    "\n",
    "**CONTRO**:\n",
    "- Meno controllo\n",
    "- Possibili bias ereditati\n",
    "- Licenze da rispettare\n",
    "\n",
    "### Alternative moderne:\n",
    "\n",
    "1. **Fine-tuning**: adatta modello esistente a task specifico\n",
    "2. **LoRA/QLoRA**: fine-tuning efficiente con pochi parametri\n",
    "3. **RAG (Retrieval-Augmented Generation)**: combina modello + database\n",
    "4. **Prompt engineering**: ottimizza prompt senza training\n",
    "5. **Few-shot learning**: insegna con pochi esempi\n",
    "\n",
    "### Considerazioni aziendali:\n",
    "\n",
    "- **Startup**: quasi sempre riuso + fine-tuning\n",
    "- **Big Tech**: possono permettersi pre-training (Google, Meta, OpenAI)\n",
    "- **Ricerca**: pre-training per esplorare nuove architetture\n",
    "- **Produzione**: riuso + ottimizzazioni (quantizzazione, distillazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stima costi pre-training (ordine di grandezza)\n",
    "cost_estimates = {\n",
    "    'Modello': ['GPT-2 small', 'GPT-2 medium', 'GPT-3', 'LLaMA-7B', 'LLaMA-65B'],\n",
    "    'Parametri': ['117M', '345M', '175B', '7B', '65B'],\n",
    "    'Costo Compute (USD)': ['~$50K', '~$200K', '~$4.6M', '~$150K', '~$2.5M'],\n",
    "    'Tempo (GPU-giorni)': ['~100', '~300', '~10,000', '~500', '~5,000'],\n",
    "    'CO2 (tonnellate)': ['~5', '~15', '~500', '~25', '~250']\n",
    "}\n",
    "\n",
    "df_costs = pd.DataFrame(cost_estimates)\n",
    "\n",
    "print(\"\\nüí∞ Stima Costi Pre-training:\\n\")\n",
    "print(df_costs.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note:\")\n",
    "print(\"   - Stime approssimative basate su letteratura pubblica\")\n",
    "print(\"   - Costi variano con hardware e ottimizzazioni\")\n",
    "print(\"   - Impatto ambientale significativo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Esercizi TODO\n",
    "\n",
    "### Esercizio 1: Testa GPT-2 large\n",
    "Carica GPT-2 large (762M parametri) e confronta con i modelli gi√† testati. √à gestibile su CPU?\n",
    "\n",
    "### Esercizio 2: Analisi vocabolario\n",
    "Confronta la dimensione del vocabolario dei tre tokenizer. Sono identici?\n",
    "\n",
    "### Esercizio 3: Batch inference\n",
    "Misura il tempo di inferenza con batch size diversi (1, 2, 4, 8). Come scala?\n",
    "\n",
    "### Esercizio 4: Temperature sweep\n",
    "Genera testo con temperature diverse (0.1, 0.5, 1.0, 1.5) e osserva le differenze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 1 - Testa GPT-2 large\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 2 - Analisi vocabolario\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 3 - Batch inference\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Esercizio 4 - Temperature sweep\n",
    "# Scrivi qui il tuo codice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Soluzioni degli esercizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUZIONE Esercizio 1: Testa GPT-2 large\n",
    "\n",
    "print(\"‚è≥ Caricamento GPT-2 large (774M parametri)...\")\n",
    "print(\"‚ö†Ô∏è  ATTENZIONE: Richiede ~3GB RAM e download ~3GB\")\n",
    "\n",
    "# Decommentare per eseguire (richiede tempo e risorse)\n",
    "# tokenizer_large = AutoTokenizer.from_pretrained('gpt2-large')\n",
    "# model_large = AutoModelForCausalLM.from_pretrained('gpt2-large')\n",
    "# model_large.eval()\n",
    "\n",
    "# Test generazione\n",
    "# test_prompt = \"The future of artificial intelligence\"\n",
    "# output_large = generate_text(model_large, tokenizer_large, test_prompt, max_new_tokens=50)\n",
    "# print(f\"\\nGPT-2 large output:\\n{output_large}\")\n",
    "\n",
    "# Analisi parametri\n",
    "# num_params_large = sum(p.numel() for p in model_large.parameters())\n",
    "# print(f\"\\nüìä GPT-2 large: {num_params_large/1e6:.1f}M parametri\")\n",
    "\n",
    "print(\"\\nüí° GPT-2 large ha 774M parametri (2x GPT-2 medium)\")\n",
    "print(\"   Pro: Migliore qualit√† output\")\n",
    "print(\"   Contro: Richiede molta RAM e tempo di inferenza\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUZIONE Esercizio 2: Analisi vocabolario\n",
    "\n",
    "print(\"üìä Analisi Vocabolario:\\n\")\n",
    "\n",
    "for model_key, model_name in MODEL_NAMES.items():\n",
    "    tokenizer = tokenizers[model_key]\n",
    "    vocab_size = len(tokenizer)\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"   Vocab size: {vocab_size}\")\n",
    "    print(f\"   PAD token: {tokenizer.pad_token}\")\n",
    "    print(f\"   EOS token: {tokenizer.eos_token}\")\n",
    "    print(f\"   BOS token: {tokenizer.bos_token}\\n\")\n",
    "\n",
    "print(\"üí° Tutti i modelli GPT-2 usano lo stesso tokenizer (BPE con 50257 token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUZIONE Esercizio 3: Batch inference\n",
    "\n",
    "batch_prompts = [\n",
    "    \"Machine learning is\",\n",
    "    \"Deep learning enables\",\n",
    "    \"Natural language processing\",\n",
    "    \"Computer vision applications\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Batch Inference Test:\\n\")\n",
    "\n",
    "# Tokenizza batch\n",
    "inputs_batch = tokenizers['gpt2'](batch_prompts, return_tensors='pt', padding=True)\n",
    "\n",
    "# Genera per tutto il batch\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_batch = models['gpt2'].generate(\n",
    "        **inputs_batch,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizers['gpt2'].eos_token_id\n",
    "    )\n",
    "\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "# Decodifica\n",
    "for i, (prompt, output_ids) in enumerate(zip(batch_prompts, outputs_batch)):\n",
    "    output_text = tokenizers['gpt2'].decode(output_ids, skip_special_tokens=True)\n",
    "    print(f\"{i+1}. {output_text}\\n\")\n",
    "\n",
    "print(f\"‚è±Ô∏è Tempo batch ({len(batch_prompts)} prompts): {batch_time:.2f}s\")\n",
    "print(f\"   Tempo medio per prompt: {batch_time/len(batch_prompts):.2f}s\")\n",
    "\n",
    "# Confronto con inferenza sequenziale\n",
    "start_time = time.time()\n",
    "for prompt in batch_prompts:\n",
    "    _ = generate_text(models['gpt2'], tokenizers['gpt2'], prompt, max_new_tokens=30)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüìä Confronto:\")\n",
    "print(f\"   Batch: {batch_time:.2f}s\")\n",
    "print(f\"   Sequential: {sequential_time:.2f}s\")\n",
    "print(f\"   Speedup: {sequential_time/batch_time:.2f}x\")\n",
    "\n",
    "print(\"\\nüí° Batch inference √® pi√π efficiente quando si processano molti prompt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUZIONE Esercizio 4: Temperature sweep\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5]\n",
    "test_prompt = \"Artificial intelligence is\"\n",
    "\n",
    "print(\"üå°Ô∏è Temperature Sweep:\\n\")\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    \n",
    "    output = generate_text(\n",
    "        models['gpt2'],\n",
    "        tokenizers['gpt2'],\n",
    "        test_prompt,\n",
    "        max_new_tokens=30,\n",
    "        temperature=temp\n",
    "    )\n",
    "    \n",
    "    generated = output[len(test_prompt):].strip()\n",
    "    print(f\"   {generated}\\n\")\n",
    "\n",
    "print(\"üí° Osservazioni:\")\n",
    "print(\"   - Temperature bassa (0.1): output deterministico, ripetitivo\")\n",
    "print(\"   - Temperature media (0.7-1.0): buon bilanciamento\")\n",
    "print(\"   - Temperature alta (1.5+): pi√π creativo ma meno coerente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Conclusione Day 1\n",
    "\n",
    "Oggi abbiamo:\n",
    "1. ‚úÖ Compreso le **scaling laws** e loro implicazioni\n",
    "2. ‚úÖ Analizzato esempi concreti: **GPT-3, LLaMA, Falcon**\n",
    "3. ‚úÖ Caricato e confrontato **3 modelli** di dimensioni diverse\n",
    "4. ‚úÖ Analizzato **parametri, memoria, velocit√†**\n",
    "5. ‚úÖ Confrontato **qualit√† output** su prompt identici\n",
    "6. ‚úÖ Discusso **trade-off** e quando riusare vs pre-training\n",
    "\n",
    "**Prossimi passi (Day 2)**:\n",
    "- Supervised Fine-Tuning (SFT)\n",
    "- LoRA e QLoRA per fine-tuning efficiente\n",
    "- Dataset IMDB per classification\n",
    "- Confronto base vs fine-tuned\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ File generati\n",
    "\n",
    "```\n",
    "./results_day1/\n",
    "‚îú‚îÄ‚îÄ scaling_laws.png\n",
    "‚îú‚îÄ‚îÄ models_comparison.csv\n",
    "‚îú‚îÄ‚îÄ model_parameters.png\n",
    "‚îú‚îÄ‚îÄ generation_outputs.csv\n",
    "‚îî‚îÄ‚îÄ inference_benchmark.png\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
