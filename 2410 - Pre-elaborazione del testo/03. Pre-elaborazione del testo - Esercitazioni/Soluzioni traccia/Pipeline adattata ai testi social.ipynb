{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4d6d9d-ba1f-4631-84a4-dd92fe1fbb4b",
   "metadata": {},
   "source": [
    "# Importazione librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d8e987-eaca-4e6b-9a9a-a8d44082fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importazione librerie completata\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(\"Importazione librerie completata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd0413-77fd-4553-832a-47cd0b26fb3d",
   "metadata": {},
   "source": [
    "# Pipeline social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b29ad1-a59d-4e41-ba7c-cb02d68d703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineSocial:\n",
    "    def __init__(self, lingua='italian'):\n",
    "        \"\"\"Pipeline NLP adattata a testi social (emoji, slang, hashtag)\"\"\"\n",
    "        self.lingua = lingua\n",
    "        self.nlp = spacy.load('it_core_news_sm' if lingua == 'italian' else 'en_core_web_sm')\n",
    "        self.stemmer = SnowballStemmer(lingua)\n",
    "        self.stop_words = set(stopwords.words(lingua))\n",
    "\n",
    "        # Dizionario di sostituzioni per emoji e slang comuni\n",
    "        self.mappa_semantica = {\n",
    "            \"😱\": \"sorpresa\",\n",
    "            \"🔥\": \"fantastico\",\n",
    "            \"😂\": \"divertente\",\n",
    "            \"😎\": \"cool\",\n",
    "            \"😍\": \"adorabile\",\n",
    "            \"lol\": \"divertente\",\n",
    "            \"omg\": \"incredibile\",\n",
    "            \"wtf\": \"assurdo\",\n",
    "            \"top\": \"eccellente\"\n",
    "        }\n",
    "\n",
    "    def normalizza_testo(self, testo):\n",
    "        \"\"\"Normalizzazione semantica (emoji, hashtag, slang, lowercasing)\"\"\"\n",
    "        testo = testo.lower()\n",
    "\n",
    "        # Sostituisci emoji e slang tramite dizionario\n",
    "        for chiave, sostituto in self.mappa_semantica.items():\n",
    "            testo = testo.replace(chiave, f\" {sostituto} \")\n",
    "\n",
    "        # Espandi hashtag (mantieni la parola chiave)\n",
    "        testo = re.sub(r'#(\\w+)', r'\\1', testo)\n",
    "\n",
    "        # Rimuovi mention tipo @username\n",
    "        testo = re.sub(r'@\\w+', '', testo)\n",
    "\n",
    "        # Rimuovi URL ed email\n",
    "        testo = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', '', testo)\n",
    "\n",
    "        # Rimuovi caratteri non alfanumerici (tranne apostrofi e spazi)\n",
    "        testo = re.sub(r\"[^a-zàèéìòù'\\s]\", ' ', testo)\n",
    "\n",
    "        # Rimuovi spazi multipli\n",
    "        testo = re.sub(r'\\s+', ' ', testo).strip()\n",
    "        return testo\n",
    "\n",
    "    def tokenizza_e_lemmatizza(self, testo):\n",
    "        \"\"\"Tokenizzazione + Lemmatizzazione\"\"\"\n",
    "        doc = self.nlp(testo)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_space and len(token.text) > 2:\n",
    "                if token.lemma_ != '-PRON-':\n",
    "                    tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    tokens.append(self.stemmer.stem(token.text))\n",
    "        return tokens\n",
    "\n",
    "    def rimuovi_stop_words(self, tokens):\n",
    "        \"\"\"Rimuove stopwords mantenendo parole chiave\"\"\"\n",
    "        return [t for t in tokens if t not in self.stop_words]\n",
    "\n",
    "    def processa_testo(self, testo):\n",
    "        \"\"\"Esegue tutti gli step\"\"\"\n",
    "        testo_norm = self.normalizza_testo(testo)\n",
    "        tokens = self.tokenizza_e_lemmatizza(testo_norm)\n",
    "        tokens = self.rimuovi_stop_words(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24bdd0-71e0-47dc-b7b1-e7f2ec749620",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448cd279-0199-4bd1-8255-417361b2c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔤 Tokens finali: ['incredibile', 'sorpreso', 'credere', 'nuovo', 'stagione', 'strangere', 'things', 'pazzesca', 'fantastico', 'fantastico', 'attore', 'sempre', 'eccellente', 'trama', 'stavolta', 'assurdo', 'divertente', 'divertente', 'netflix', 'strangerthings']\n"
     ]
    }
   ],
   "source": [
    "# Test del caso specifico\n",
    "testo = \"OMG 😱 non ci credo, la nuova stagione di Stranger Things è PAZZESCA!!! 🔥🔥 Gli attori sempre top, ma la trama stavolta è assurda lol 😂 #Netflix #strangerthings\"\n",
    "\n",
    "pipeline = PipelineSocial()\n",
    "risultato = pipeline.processa_testo(testo)\n",
    "\n",
    "print(\"🔤 Tokens finali:\", risultato)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b5099-6afb-4a47-b111-b254db75e7ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Approccio utilizzato\n",
    "- Creazione di un dizionario per tradurre slang ed emoji comuni in parole chiave descrittive;\n",
    "- Sostituituzione degli hashtag/mention preservando termini utili (es. #Netflix → netflix);\n",
    "- Pulizia più robusta per emoji e simboli ripetuti;\n",
    "- Mantenimento dell’approccio spaCy + NLTK per tokenizzazione, lemmatizzazione e stopwords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
