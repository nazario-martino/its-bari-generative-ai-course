{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4d6d9d-ba1f-4631-84a4-dd92fe1fbb4b",
   "metadata": {},
   "source": [
    "# Importazione librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d8e987-eaca-4e6b-9a9a-a8d44082fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importazione librerie completata\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(\"Importazione librerie completata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd0413-77fd-4553-832a-47cd0b26fb3d",
   "metadata": {},
   "source": [
    "# Pipeline articolo di giornale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b29ad1-a59d-4e41-ba7c-cb02d68d703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineNotizia:\n",
    "    def __init__(self, lingua='italian'):\n",
    "        \"\"\"Pipeline NLP per articoli di notizie\"\"\"\n",
    "        self.lingua = lingua\n",
    "        self.nlp = spacy.load('it_core_news_sm' if lingua == 'italian' else 'en_core_web_sm')\n",
    "        self.stemmer = SnowballStemmer(lingua)\n",
    "        self.stop_words = set(stopwords.words(lingua))\n",
    "        \n",
    "    def normalizza_testo(self, testo):\n",
    "        \"\"\"Normalizzazione leggera: rimuove punteggiatura ma preserva maiuscole\"\"\"\n",
    "        testo = re.sub(r'[^\\w\\sÃ Ã¨Ã©Ã¬Ã²Ã¹]', ' ', testo)  # Rimuove punteggiatura\n",
    "        testo = re.sub(r'\\s+', ' ', testo).strip()\n",
    "        return testo\n",
    "    \n",
    "    def tokenizza_e_lemmatizza(self, testo):\n",
    "        \"\"\"Tokenizza e lemmatizza\"\"\"\n",
    "        doc = self.nlp(testo)\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not token.is_punct and not token.is_space and token.is_alpha:\n",
    "                lemma = token.lemma_ if token.lemma_ != '-PRON-' else token.text\n",
    "                tokens.append((token.text, lemma, token.pos_))\n",
    "        return tokens\n",
    "    \n",
    "    def estrai_entita(self, testo):\n",
    "        \"\"\"Estrae Named Entities dal testo (es. organizzazioni, luoghi, ecc.)\"\"\"\n",
    "        doc = self.nlp(testo)\n",
    "        entita = [ent.text for ent in doc.ents]\n",
    "        return entita\n",
    "    \n",
    "    def rimuovi_stop_words(self, tokens):\n",
    "        \"\"\"Rimuove stopwords, mantenendo solo parole chiave\"\"\"\n",
    "        return [lemma for (text, lemma, pos) in tokens if lemma.lower() not in self.stop_words]\n",
    "    \n",
    "    def filtra_per_pos(self, tokens):\n",
    "        \"\"\"Mantiene solo sostantivi e verbi principali\"\"\"\n",
    "        return [lemma for (text, lemma, pos) in tokens if pos in ['NOUN', 'VERB']]\n",
    "    \n",
    "    def processa_testo(self, testo, usa_ner=True):\n",
    "        \"\"\"Pipeline completa\"\"\"\n",
    "        testo_norm = self.normalizza_testo(testo)\n",
    "        tokens = self.tokenizza_e_lemmatizza(testo_norm)\n",
    "        \n",
    "        # Rimozione stopwords e filtraggio per parte del discorso\n",
    "        tokens_filtrati = self.rimuovi_stop_words(tokens)\n",
    "        tokens_filtrati = [lemma for (text, lemma, pos) in tokens if lemma in tokens_filtrati and pos in ['NOUN', 'VERB']]\n",
    "        \n",
    "        return {\n",
    "            'tokens_chiave': list(set(tokens_filtrati)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24bdd0-71e0-47dc-b7b1-e7f2ec749620",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "448cd279-0199-4bd1-8255-417361b2c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“° Tokens chiave: ['impresa', 'incentivo', 'sostenere', 'annunciare', 'governo', 'settore', 'piano']\n"
     ]
    }
   ],
   "source": [
    "# Test 3 â€” Articolo di Notizia\n",
    "testo3 = \"Il governo ha annunciato oggi un nuovo piano di incentivi per sostenere le piccole e medie imprese nel settore tecnologico.\"\n",
    "\n",
    "pipeline_news = PipelineNotizia()\n",
    "risultato3 = pipeline_news.processa_testo(testo3)\n",
    "\n",
    "print(\"ðŸ“° Tokens chiave:\", risultato3['tokens_chiave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b5099-6afb-4a47-b111-b254db75e7ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Approccio utilizzato\n",
    "Per un articolo di notizia, lâ€™obiettivo Ã¨ isolare le informazioni chiave (soggetti, azioni, oggetti) e riconoscere entitÃ  (istituzioni, aziende, ecc.).\n",
    "Quindi la pipeline deve:\n",
    "- mantenere il testo pulito ma senza perdere i nomi propri;\n",
    "- filtrare i sostantivi e verbi principali, perchÃ© in un articolo sono i portatori di significato."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
